{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.7907542579075426,
  "eval_steps": 500,
  "global_step": 3900,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0040551500405515,
      "grad_norm": 6.399161338806152,
      "learning_rate": 3.6e-05,
      "loss": 4.4264,
      "step": 20
    },
    {
      "epoch": 0.008110300081103,
      "grad_norm": 2.0963056087493896,
      "learning_rate": 7.6e-05,
      "loss": 3.182,
      "step": 40
    },
    {
      "epoch": 0.012165450121654502,
      "grad_norm": 1.4260926246643066,
      "learning_rate": 0.000116,
      "loss": 2.2233,
      "step": 60
    },
    {
      "epoch": 0.016220600162206,
      "grad_norm": 1.8676693439483643,
      "learning_rate": 0.00015600000000000002,
      "loss": 2.0205,
      "step": 80
    },
    {
      "epoch": 0.0202757502027575,
      "grad_norm": 1.1541178226470947,
      "learning_rate": 0.000196,
      "loss": 1.8906,
      "step": 100
    },
    {
      "epoch": 0.024330900243309004,
      "grad_norm": 1.0970077514648438,
      "learning_rate": 0.00019999315212298514,
      "loss": 1.8347,
      "step": 120
    },
    {
      "epoch": 0.028386050283860504,
      "grad_norm": 1.540643334388733,
      "learning_rate": 0.0001999694816534324,
      "loss": 1.811,
      "step": 140
    },
    {
      "epoch": 0.032441200324412,
      "grad_norm": 1.1192973852157593,
      "learning_rate": 0.0001999289080509696,
      "loss": 1.7444,
      "step": 160
    },
    {
      "epoch": 0.0364963503649635,
      "grad_norm": 1.1044353246688843,
      "learning_rate": 0.00019987143817590036,
      "loss": 1.741,
      "step": 180
    },
    {
      "epoch": 0.040551500405515,
      "grad_norm": 0.9813517332077026,
      "learning_rate": 0.00019979708174539952,
      "loss": 1.7267,
      "step": 200
    },
    {
      "epoch": 0.04460665044606651,
      "grad_norm": 1.0675498247146606,
      "learning_rate": 0.00019970585133187032,
      "loss": 1.7527,
      "step": 220
    },
    {
      "epoch": 0.04866180048661801,
      "grad_norm": 1.0475963354110718,
      "learning_rate": 0.0001995977623608184,
      "loss": 1.7502,
      "step": 240
    },
    {
      "epoch": 0.05271695052716951,
      "grad_norm": 0.8654850721359253,
      "learning_rate": 0.00019947283310824368,
      "loss": 1.7766,
      "step": 260
    },
    {
      "epoch": 0.05677210056772101,
      "grad_norm": 0.8243007659912109,
      "learning_rate": 0.00019933108469755032,
      "loss": 1.7372,
      "step": 280
    },
    {
      "epoch": 0.06082725060827251,
      "grad_norm": 1.089294195175171,
      "learning_rate": 0.00019917254109597498,
      "loss": 1.6694,
      "step": 300
    },
    {
      "epoch": 0.064882400648824,
      "grad_norm": 0.8965266346931458,
      "learning_rate": 0.00019899722911053438,
      "loss": 1.7306,
      "step": 320
    },
    {
      "epoch": 0.0689375506893755,
      "grad_norm": 0.8676117062568665,
      "learning_rate": 0.00019880517838349282,
      "loss": 1.7473,
      "step": 340
    },
    {
      "epoch": 0.072992700729927,
      "grad_norm": 0.8659130930900574,
      "learning_rate": 0.00019859642138734995,
      "loss": 1.6566,
      "step": 360
    },
    {
      "epoch": 0.0770478507704785,
      "grad_norm": 1.0689061880111694,
      "learning_rate": 0.0001983709934193505,
      "loss": 1.7589,
      "step": 380
    },
    {
      "epoch": 0.08110300081103,
      "grad_norm": 0.9314943552017212,
      "learning_rate": 0.0001981289325955158,
      "loss": 1.7412,
      "step": 400
    },
    {
      "epoch": 0.0851581508515815,
      "grad_norm": 0.9400247931480408,
      "learning_rate": 0.00019787027984419936,
      "loss": 1.7102,
      "step": 420
    },
    {
      "epoch": 0.08921330089213302,
      "grad_norm": 0.7713639736175537,
      "learning_rate": 0.00019759507889916632,
      "loss": 1.7174,
      "step": 440
    },
    {
      "epoch": 0.09326845093268452,
      "grad_norm": 0.9931594729423523,
      "learning_rate": 0.00019730337629219886,
      "loss": 1.6686,
      "step": 460
    },
    {
      "epoch": 0.09732360097323602,
      "grad_norm": 0.8418185710906982,
      "learning_rate": 0.00019699522134522865,
      "loss": 1.6696,
      "step": 480
    },
    {
      "epoch": 0.10137875101378752,
      "grad_norm": 0.8551792502403259,
      "learning_rate": 0.00019667066616199712,
      "loss": 1.7243,
      "step": 500
    },
    {
      "epoch": 0.10543390105433902,
      "grad_norm": 0.8549308180809021,
      "learning_rate": 0.00019632976561924573,
      "loss": 1.687,
      "step": 520
    },
    {
      "epoch": 0.10948905109489052,
      "grad_norm": 0.717846691608429,
      "learning_rate": 0.00019597257735743718,
      "loss": 1.6896,
      "step": 540
    },
    {
      "epoch": 0.11354420113544202,
      "grad_norm": 0.9293779730796814,
      "learning_rate": 0.00019559916177100956,
      "loss": 1.7312,
      "step": 560
    },
    {
      "epoch": 0.11759935117599352,
      "grad_norm": 0.9265512824058533,
      "learning_rate": 0.00019520958199816449,
      "loss": 1.7001,
      "step": 580
    },
    {
      "epoch": 0.12165450121654502,
      "grad_norm": 0.7644781470298767,
      "learning_rate": 0.00019480390391019153,
      "loss": 1.7236,
      "step": 600
    },
    {
      "epoch": 0.12570965125709652,
      "grad_norm": 0.9200144410133362,
      "learning_rate": 0.00019438219610033063,
      "loss": 1.6612,
      "step": 620
    },
    {
      "epoch": 0.129764801297648,
      "grad_norm": 0.9818695783615112,
      "learning_rate": 0.0001939445298721741,
      "loss": 1.6936,
      "step": 640
    },
    {
      "epoch": 0.13381995133819952,
      "grad_norm": 0.8658899068832397,
      "learning_rate": 0.00019349097922761023,
      "loss": 1.6608,
      "step": 660
    },
    {
      "epoch": 0.137875101378751,
      "grad_norm": 0.8663012385368347,
      "learning_rate": 0.00019302162085431125,
      "loss": 1.7104,
      "step": 680
    },
    {
      "epoch": 0.14193025141930252,
      "grad_norm": 0.6926597952842712,
      "learning_rate": 0.0001925365341127662,
      "loss": 1.7236,
      "step": 700
    },
    {
      "epoch": 0.145985401459854,
      "grad_norm": 0.821912407875061,
      "learning_rate": 0.00019203580102286288,
      "loss": 1.7089,
      "step": 720
    },
    {
      "epoch": 0.15004055150040552,
      "grad_norm": 1.1358879804611206,
      "learning_rate": 0.00019151950625001955,
      "loss": 1.6853,
      "step": 740
    },
    {
      "epoch": 0.154095701540957,
      "grad_norm": 0.8431806564331055,
      "learning_rate": 0.0001909877370908693,
      "loss": 1.7183,
      "step": 760
    },
    {
      "epoch": 0.15815085158150852,
      "grad_norm": 0.8078606128692627,
      "learning_rate": 0.00019044058345849986,
      "loss": 1.7171,
      "step": 780
    },
    {
      "epoch": 0.16220600162206,
      "grad_norm": 0.8555957674980164,
      "learning_rate": 0.0001898781378672508,
      "loss": 1.6727,
      "step": 800
    },
    {
      "epoch": 0.16626115166261152,
      "grad_norm": 0.8178600072860718,
      "learning_rate": 0.00018930049541707086,
      "loss": 1.6485,
      "step": 820
    },
    {
      "epoch": 0.170316301703163,
      "grad_norm": 0.8160397410392761,
      "learning_rate": 0.0001887077537774383,
      "loss": 1.6719,
      "step": 840
    },
    {
      "epoch": 0.17437145174371452,
      "grad_norm": 0.6942429542541504,
      "learning_rate": 0.00018810001317084648,
      "loss": 1.6702,
      "step": 860
    },
    {
      "epoch": 0.17842660178426603,
      "grad_norm": 0.9157407879829407,
      "learning_rate": 0.00018747737635585815,
      "loss": 1.7278,
      "step": 880
    },
    {
      "epoch": 0.18248175182481752,
      "grad_norm": 0.7684374451637268,
      "learning_rate": 0.00018683994860973054,
      "loss": 1.684,
      "step": 900
    },
    {
      "epoch": 0.18653690186536903,
      "grad_norm": 0.7664427161216736,
      "learning_rate": 0.00018618783771061486,
      "loss": 1.7287,
      "step": 920
    },
    {
      "epoch": 0.19059205190592052,
      "grad_norm": 0.8498724102973938,
      "learning_rate": 0.0001855211539193329,
      "loss": 1.6199,
      "step": 940
    },
    {
      "epoch": 0.19464720194647203,
      "grad_norm": 0.852802038192749,
      "learning_rate": 0.0001848400099607338,
      "loss": 1.6731,
      "step": 960
    },
    {
      "epoch": 0.19870235198702352,
      "grad_norm": 0.7934236526489258,
      "learning_rate": 0.00018414452100463407,
      "loss": 1.6775,
      "step": 980
    },
    {
      "epoch": 0.20275750202757503,
      "grad_norm": 0.8844372630119324,
      "learning_rate": 0.0001834348046463445,
      "loss": 1.6909,
      "step": 1000
    },
    {
      "epoch": 0.20681265206812652,
      "grad_norm": 0.7467173933982849,
      "learning_rate": 0.00018271098088678666,
      "loss": 1.682,
      "step": 1020
    },
    {
      "epoch": 0.21086780210867803,
      "grad_norm": 0.8553664088249207,
      "learning_rate": 0.000181973172112203,
      "loss": 1.6504,
      "step": 1040
    },
    {
      "epoch": 0.21492295214922952,
      "grad_norm": 0.7749987840652466,
      "learning_rate": 0.0001812215030734632,
      "loss": 1.7084,
      "step": 1060
    },
    {
      "epoch": 0.21897810218978103,
      "grad_norm": 0.7557559609413147,
      "learning_rate": 0.000180456100864971,
      "loss": 1.6816,
      "step": 1080
    },
    {
      "epoch": 0.22303325223033252,
      "grad_norm": 0.898171603679657,
      "learning_rate": 0.00017967709490317475,
      "loss": 1.7075,
      "step": 1100
    },
    {
      "epoch": 0.22708840227088403,
      "grad_norm": 0.8288281559944153,
      "learning_rate": 0.0001788846169046853,
      "loss": 1.7207,
      "step": 1120
    },
    {
      "epoch": 0.23114355231143552,
      "grad_norm": 0.7779068350791931,
      "learning_rate": 0.00017807880086400494,
      "loss": 1.5719,
      "step": 1140
    },
    {
      "epoch": 0.23519870235198703,
      "grad_norm": 0.8410137295722961,
      "learning_rate": 0.00017725978303087116,
      "loss": 1.6743,
      "step": 1160
    },
    {
      "epoch": 0.23925385239253852,
      "grad_norm": 0.8810112476348877,
      "learning_rate": 0.00017642770188721932,
      "loss": 1.677,
      "step": 1180
    },
    {
      "epoch": 0.24330900243309003,
      "grad_norm": 0.775789201259613,
      "learning_rate": 0.00017558269812376745,
      "loss": 1.6361,
      "step": 1200
    },
    {
      "epoch": 0.24736415247364152,
      "grad_norm": 0.7573976516723633,
      "learning_rate": 0.00017472491461622812,
      "loss": 1.7026,
      "step": 1220
    },
    {
      "epoch": 0.25141930251419303,
      "grad_norm": 0.9683657288551331,
      "learning_rate": 0.00017385449640115047,
      "loss": 1.6594,
      "step": 1240
    },
    {
      "epoch": 0.25547445255474455,
      "grad_norm": 0.8207905292510986,
      "learning_rate": 0.000172971590651397,
      "loss": 1.6464,
      "step": 1260
    },
    {
      "epoch": 0.259529602595296,
      "grad_norm": 0.9187310338020325,
      "learning_rate": 0.00017207634665125906,
      "loss": 1.723,
      "step": 1280
    },
    {
      "epoch": 0.2635847526358475,
      "grad_norm": 0.7274195551872253,
      "learning_rate": 0.00017116891577121577,
      "loss": 1.7211,
      "step": 1300
    },
    {
      "epoch": 0.26763990267639903,
      "grad_norm": 0.8157933950424194,
      "learning_rate": 0.00017024945144233938,
      "loss": 1.6313,
      "step": 1320
    },
    {
      "epoch": 0.27169505271695055,
      "grad_norm": 0.980732262134552,
      "learning_rate": 0.00016931810913035306,
      "loss": 1.7334,
      "step": 1340
    },
    {
      "epoch": 0.275750202757502,
      "grad_norm": 0.9515752792358398,
      "learning_rate": 0.00016837504630934412,
      "loss": 1.7344,
      "step": 1360
    },
    {
      "epoch": 0.2798053527980535,
      "grad_norm": 0.8487598896026611,
      "learning_rate": 0.00016742042243513797,
      "loss": 1.6444,
      "step": 1380
    },
    {
      "epoch": 0.28386050283860503,
      "grad_norm": 0.7430022954940796,
      "learning_rate": 0.0001664543989183366,
      "loss": 1.6239,
      "step": 1400
    },
    {
      "epoch": 0.28791565287915655,
      "grad_norm": 0.8847016096115112,
      "learning_rate": 0.00016547713909702717,
      "loss": 1.6588,
      "step": 1420
    },
    {
      "epoch": 0.291970802919708,
      "grad_norm": 0.8690552711486816,
      "learning_rate": 0.000164488808209164,
      "loss": 1.6692,
      "step": 1440
    },
    {
      "epoch": 0.2960259529602595,
      "grad_norm": 0.8046749234199524,
      "learning_rate": 0.00016348957336462982,
      "loss": 1.638,
      "step": 1460
    },
    {
      "epoch": 0.30008110300081103,
      "grad_norm": 0.7875950336456299,
      "learning_rate": 0.00016247960351698038,
      "loss": 1.6389,
      "step": 1480
    },
    {
      "epoch": 0.30413625304136255,
      "grad_norm": 0.7762024998664856,
      "learning_rate": 0.00016145906943487706,
      "loss": 1.6571,
      "step": 1500
    },
    {
      "epoch": 0.308191403081914,
      "grad_norm": 0.859844982624054,
      "learning_rate": 0.00016042814367321311,
      "loss": 1.7138,
      "step": 1520
    },
    {
      "epoch": 0.3122465531224655,
      "grad_norm": 0.8479313850402832,
      "learning_rate": 0.00015938700054393738,
      "loss": 1.6636,
      "step": 1540
    },
    {
      "epoch": 0.31630170316301703,
      "grad_norm": 0.8731727600097656,
      "learning_rate": 0.00015833581608658108,
      "loss": 1.63,
      "step": 1560
    },
    {
      "epoch": 0.32035685320356855,
      "grad_norm": 0.698237419128418,
      "learning_rate": 0.0001572747680384927,
      "loss": 1.6957,
      "step": 1580
    },
    {
      "epoch": 0.32441200324412,
      "grad_norm": 0.8812178373336792,
      "learning_rate": 0.0001562040358047855,
      "loss": 1.6782,
      "step": 1600
    },
    {
      "epoch": 0.3284671532846715,
      "grad_norm": 0.876654326915741,
      "learning_rate": 0.0001551238004280033,
      "loss": 1.6006,
      "step": 1620
    },
    {
      "epoch": 0.33252230332522303,
      "grad_norm": 0.9605105519294739,
      "learning_rate": 0.0001540342445575091,
      "loss": 1.6671,
      "step": 1640
    },
    {
      "epoch": 0.33657745336577455,
      "grad_norm": 0.8231251835823059,
      "learning_rate": 0.00015293555241860233,
      "loss": 1.6587,
      "step": 1660
    },
    {
      "epoch": 0.340632603406326,
      "grad_norm": 0.8030878305435181,
      "learning_rate": 0.00015182790978136946,
      "loss": 1.6255,
      "step": 1680
    },
    {
      "epoch": 0.3446877534468775,
      "grad_norm": 0.9665604829788208,
      "learning_rate": 0.00015071150392927351,
      "loss": 1.6718,
      "step": 1700
    },
    {
      "epoch": 0.34874290348742903,
      "grad_norm": 0.8557950854301453,
      "learning_rate": 0.0001495865236274874,
      "loss": 1.6444,
      "step": 1720
    },
    {
      "epoch": 0.35279805352798055,
      "grad_norm": 0.828907310962677,
      "learning_rate": 0.00014845315909097723,
      "loss": 1.6497,
      "step": 1740
    },
    {
      "epoch": 0.35685320356853206,
      "grad_norm": 0.7361765503883362,
      "learning_rate": 0.00014731160195234012,
      "loss": 1.6725,
      "step": 1760
    },
    {
      "epoch": 0.3609083536090835,
      "grad_norm": 0.7813496589660645,
      "learning_rate": 0.00014616204522940226,
      "loss": 1.6158,
      "step": 1780
    },
    {
      "epoch": 0.36496350364963503,
      "grad_norm": 0.7641428709030151,
      "learning_rate": 0.00014500468329258306,
      "loss": 1.6923,
      "step": 1800
    },
    {
      "epoch": 0.36901865369018655,
      "grad_norm": 0.8490419387817383,
      "learning_rate": 0.00014383971183203035,
      "loss": 1.7128,
      "step": 1820
    },
    {
      "epoch": 0.37307380373073806,
      "grad_norm": 0.7554851770401001,
      "learning_rate": 0.00014266732782453252,
      "loss": 1.6772,
      "step": 1840
    },
    {
      "epoch": 0.3771289537712895,
      "grad_norm": 0.7608698606491089,
      "learning_rate": 0.0001414877295002131,
      "loss": 1.6768,
      "step": 1860
    },
    {
      "epoch": 0.38118410381184104,
      "grad_norm": 0.8109844923019409,
      "learning_rate": 0.00014030111630901354,
      "loss": 1.6286,
      "step": 1880
    },
    {
      "epoch": 0.38523925385239255,
      "grad_norm": 0.8966554999351501,
      "learning_rate": 0.0001391076888869694,
      "loss": 1.6183,
      "step": 1900
    },
    {
      "epoch": 0.38929440389294406,
      "grad_norm": 0.7370070219039917,
      "learning_rate": 0.0001379076490222865,
      "loss": 1.6555,
      "step": 1920
    },
    {
      "epoch": 0.3933495539334955,
      "grad_norm": 0.7943277359008789,
      "learning_rate": 0.00013670119962122182,
      "loss": 1.6205,
      "step": 1940
    },
    {
      "epoch": 0.39740470397404704,
      "grad_norm": 0.9557520747184753,
      "learning_rate": 0.0001354885446737754,
      "loss": 1.6258,
      "step": 1960
    },
    {
      "epoch": 0.40145985401459855,
      "grad_norm": 0.7429898381233215,
      "learning_rate": 0.00013426988921919934,
      "loss": 1.6587,
      "step": 1980
    },
    {
      "epoch": 0.40551500405515006,
      "grad_norm": 0.842354953289032,
      "learning_rate": 0.00013304543931132908,
      "loss": 1.5806,
      "step": 2000
    },
    {
      "epoch": 0.4095701540957015,
      "grad_norm": 0.8234986662864685,
      "learning_rate": 0.00013181540198374325,
      "loss": 1.5932,
      "step": 2020
    },
    {
      "epoch": 0.41362530413625304,
      "grad_norm": 0.8185062408447266,
      "learning_rate": 0.00013057998521475767,
      "loss": 1.6568,
      "step": 2040
    },
    {
      "epoch": 0.41768045417680455,
      "grad_norm": 0.6918169260025024,
      "learning_rate": 0.00012933939789225987,
      "loss": 1.6562,
      "step": 2060
    },
    {
      "epoch": 0.42173560421735606,
      "grad_norm": 0.8213347792625427,
      "learning_rate": 0.00012809384977838988,
      "loss": 1.7334,
      "step": 2080
    },
    {
      "epoch": 0.4257907542579075,
      "grad_norm": 0.8027557730674744,
      "learning_rate": 0.0001268435514740727,
      "loss": 1.6361,
      "step": 2100
    },
    {
      "epoch": 0.42984590429845904,
      "grad_norm": 0.7506842017173767,
      "learning_rate": 0.00012558871438340952,
      "loss": 1.6387,
      "step": 2120
    },
    {
      "epoch": 0.43390105433901055,
      "grad_norm": 0.7693784832954407,
      "learning_rate": 0.00012432955067793287,
      "loss": 1.6883,
      "step": 2140
    },
    {
      "epoch": 0.43795620437956206,
      "grad_norm": 0.894321858882904,
      "learning_rate": 0.00012306627326073188,
      "loss": 1.6576,
      "step": 2160
    },
    {
      "epoch": 0.4420113544201135,
      "grad_norm": 0.7827674746513367,
      "learning_rate": 0.00012179909573045421,
      "loss": 1.6552,
      "step": 2180
    },
    {
      "epoch": 0.44606650446066504,
      "grad_norm": 0.8741680383682251,
      "learning_rate": 0.00012052823234519004,
      "loss": 1.5755,
      "step": 2200
    },
    {
      "epoch": 0.45012165450121655,
      "grad_norm": 0.6987921595573425,
      "learning_rate": 0.0001192538979862447,
      "loss": 1.6702,
      "step": 2220
    },
    {
      "epoch": 0.45417680454176806,
      "grad_norm": 0.6435449719429016,
      "learning_rate": 0.00011797630812180591,
      "loss": 1.6524,
      "step": 2240
    },
    {
      "epoch": 0.4582319545823195,
      "grad_norm": 0.8156301379203796,
      "learning_rate": 0.00011669567877051184,
      "loss": 1.6258,
      "step": 2260
    },
    {
      "epoch": 0.46228710462287104,
      "grad_norm": 0.8887466192245483,
      "learning_rate": 0.000115412226464926,
      "loss": 1.6481,
      "step": 2280
    },
    {
      "epoch": 0.46634225466342255,
      "grad_norm": 0.7241219878196716,
      "learning_rate": 0.00011412616821492525,
      "loss": 1.6076,
      "step": 2300
    },
    {
      "epoch": 0.47039740470397406,
      "grad_norm": 0.7961325645446777,
      "learning_rate": 0.00011283772147100717,
      "loss": 1.6481,
      "step": 2320
    },
    {
      "epoch": 0.4744525547445255,
      "grad_norm": 0.8389304876327515,
      "learning_rate": 0.00011154710408752294,
      "loss": 1.6596,
      "step": 2340
    },
    {
      "epoch": 0.47850770478507704,
      "grad_norm": 0.7607480883598328,
      "learning_rate": 0.00011025453428584176,
      "loss": 1.6734,
      "step": 2360
    },
    {
      "epoch": 0.48256285482562855,
      "grad_norm": 0.8720670938491821,
      "learning_rate": 0.00010896023061745348,
      "loss": 1.6629,
      "step": 2380
    },
    {
      "epoch": 0.48661800486618007,
      "grad_norm": 0.7911661863327026,
      "learning_rate": 0.00010766441192701522,
      "loss": 1.6152,
      "step": 2400
    },
    {
      "epoch": 0.4906731549067315,
      "grad_norm": 0.863778293132782,
      "learning_rate": 0.0001063672973153484,
      "loss": 1.6163,
      "step": 2420
    },
    {
      "epoch": 0.49472830494728304,
      "grad_norm": 0.8056691884994507,
      "learning_rate": 0.00010506910610239273,
      "loss": 1.6621,
      "step": 2440
    },
    {
      "epoch": 0.49878345498783455,
      "grad_norm": 0.754859447479248,
      "learning_rate": 0.00010377005779012263,
      "loss": 1.63,
      "step": 2460
    },
    {
      "epoch": 0.5028386050283861,
      "grad_norm": 0.8008590340614319,
      "learning_rate": 0.00010247037202543357,
      "loss": 1.6207,
      "step": 2480
    },
    {
      "epoch": 0.5068937550689375,
      "grad_norm": 0.7860627770423889,
      "learning_rate": 0.00010117026856300324,
      "loss": 1.6617,
      "step": 2500
    },
    {
      "epoch": 0.5109489051094891,
      "grad_norm": 0.8633927702903748,
      "learning_rate": 9.986996722813488e-05,
      "loss": 1.6057,
      "step": 2520
    },
    {
      "epoch": 0.5150040551500406,
      "grad_norm": 0.8372594714164734,
      "learning_rate": 9.856968787958867e-05,
      "loss": 1.6468,
      "step": 2540
    },
    {
      "epoch": 0.519059205190592,
      "grad_norm": 0.7994624972343445,
      "learning_rate": 9.726965037240717e-05,
      "loss": 1.6576,
      "step": 2560
    },
    {
      "epoch": 0.5231143552311436,
      "grad_norm": 0.84699946641922,
      "learning_rate": 9.597007452074175e-05,
      "loss": 1.6482,
      "step": 2580
    },
    {
      "epoch": 0.527169505271695,
      "grad_norm": 0.817937970161438,
      "learning_rate": 9.467118006068574e-05,
      "loss": 1.6454,
      "step": 2600
    },
    {
      "epoch": 0.5312246553122466,
      "grad_norm": 0.8698641061782837,
      "learning_rate": 9.337318661312072e-05,
      "loss": 1.5581,
      "step": 2620
    },
    {
      "epoch": 0.5352798053527981,
      "grad_norm": 0.7744642496109009,
      "learning_rate": 9.207631364658245e-05,
      "loss": 1.7053,
      "step": 2640
    },
    {
      "epoch": 0.5393349553933495,
      "grad_norm": 0.6848506927490234,
      "learning_rate": 9.078078044015251e-05,
      "loss": 1.6448,
      "step": 2660
    },
    {
      "epoch": 0.5433901054339011,
      "grad_norm": 0.9016058444976807,
      "learning_rate": 8.948680604638188e-05,
      "loss": 1.6013,
      "step": 2680
    },
    {
      "epoch": 0.5474452554744526,
      "grad_norm": 0.8749977350234985,
      "learning_rate": 8.819460925425297e-05,
      "loss": 1.6335,
      "step": 2700
    },
    {
      "epoch": 0.551500405515004,
      "grad_norm": 0.7865121364593506,
      "learning_rate": 8.690440855218605e-05,
      "loss": 1.6128,
      "step": 2720
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 0.7988482117652893,
      "learning_rate": 8.561642209109664e-05,
      "loss": 1.6299,
      "step": 2740
    },
    {
      "epoch": 0.559610705596107,
      "grad_norm": 0.8758953213691711,
      "learning_rate": 8.433086764750994e-05,
      "loss": 1.6404,
      "step": 2760
    },
    {
      "epoch": 0.5636658556366586,
      "grad_norm": 0.8292089104652405,
      "learning_rate": 8.304796258673845e-05,
      "loss": 1.6206,
      "step": 2780
    },
    {
      "epoch": 0.5677210056772101,
      "grad_norm": 0.7761985659599304,
      "learning_rate": 8.176792382612929e-05,
      "loss": 1.6047,
      "step": 2800
    },
    {
      "epoch": 0.5717761557177615,
      "grad_norm": 0.7820425033569336,
      "learning_rate": 8.049096779838719e-05,
      "loss": 1.6617,
      "step": 2820
    },
    {
      "epoch": 0.5758313057583131,
      "grad_norm": 0.7952873110771179,
      "learning_rate": 7.921731041497928e-05,
      "loss": 1.6101,
      "step": 2840
    },
    {
      "epoch": 0.5798864557988646,
      "grad_norm": 0.8195086717605591,
      "learning_rate": 7.794716702962832e-05,
      "loss": 1.6476,
      "step": 2860
    },
    {
      "epoch": 0.583941605839416,
      "grad_norm": 0.882430374622345,
      "learning_rate": 7.668075240189996e-05,
      "loss": 1.6651,
      "step": 2880
    },
    {
      "epoch": 0.5879967558799676,
      "grad_norm": 0.8605825901031494,
      "learning_rate": 7.541828066089057e-05,
      "loss": 1.6649,
      "step": 2900
    },
    {
      "epoch": 0.592051905920519,
      "grad_norm": 0.9370234608650208,
      "learning_rate": 7.415996526902165e-05,
      "loss": 1.6428,
      "step": 2920
    },
    {
      "epoch": 0.5961070559610706,
      "grad_norm": 0.8053000569343567,
      "learning_rate": 7.290601898594709e-05,
      "loss": 1.671,
      "step": 2940
    },
    {
      "epoch": 0.6001622060016221,
      "grad_norm": 0.8028077483177185,
      "learning_rate": 7.165665383257903e-05,
      "loss": 1.6053,
      "step": 2960
    },
    {
      "epoch": 0.6042173560421735,
      "grad_norm": 0.9007686972618103,
      "learning_rate": 7.041208105523868e-05,
      "loss": 1.6464,
      "step": 2980
    },
    {
      "epoch": 0.6082725060827251,
      "grad_norm": 0.7444444298744202,
      "learning_rate": 6.91725110899384e-05,
      "loss": 1.666,
      "step": 3000
    },
    {
      "epoch": 0.6123276561232766,
      "grad_norm": 1.0235830545425415,
      "learning_rate": 6.793815352680039e-05,
      "loss": 1.567,
      "step": 3020
    },
    {
      "epoch": 0.616382806163828,
      "grad_norm": 0.8229691982269287,
      "learning_rate": 6.670921707461862e-05,
      "loss": 1.6409,
      "step": 3040
    },
    {
      "epoch": 0.6204379562043796,
      "grad_norm": 0.7967056632041931,
      "learning_rate": 6.548590952556966e-05,
      "loss": 1.6414,
      "step": 3060
    },
    {
      "epoch": 0.624493106244931,
      "grad_norm": 0.9117998480796814,
      "learning_rate": 6.426843772007873e-05,
      "loss": 1.6015,
      "step": 3080
    },
    {
      "epoch": 0.6285482562854826,
      "grad_norm": 0.7581101059913635,
      "learning_rate": 6.305700751184643e-05,
      "loss": 1.5563,
      "step": 3100
    },
    {
      "epoch": 0.6326034063260341,
      "grad_norm": 0.8262192606925964,
      "learning_rate": 6.185182373304232e-05,
      "loss": 1.6525,
      "step": 3120
    },
    {
      "epoch": 0.6366585563665855,
      "grad_norm": 0.7612605094909668,
      "learning_rate": 6.065309015967141e-05,
      "loss": 1.6104,
      "step": 3140
    },
    {
      "epoch": 0.6407137064071371,
      "grad_norm": 0.7825772166252136,
      "learning_rate": 5.94610094771191e-05,
      "loss": 1.6318,
      "step": 3160
    },
    {
      "epoch": 0.6447688564476886,
      "grad_norm": 0.7637492418289185,
      "learning_rate": 5.827578324588049e-05,
      "loss": 1.6194,
      "step": 3180
    },
    {
      "epoch": 0.64882400648824,
      "grad_norm": 0.7323578596115112,
      "learning_rate": 5.709761186747992e-05,
      "loss": 1.6397,
      "step": 3200
    },
    {
      "epoch": 0.6528791565287916,
      "grad_norm": 0.7814502716064453,
      "learning_rate": 5.59266945505866e-05,
      "loss": 1.5957,
      "step": 3220
    },
    {
      "epoch": 0.656934306569343,
      "grad_norm": 0.8062541484832764,
      "learning_rate": 5.47632292773318e-05,
      "loss": 1.6097,
      "step": 3240
    },
    {
      "epoch": 0.6609894566098946,
      "grad_norm": 0.7761481404304504,
      "learning_rate": 5.360741276983326e-05,
      "loss": 1.6225,
      "step": 3260
    },
    {
      "epoch": 0.6650446066504461,
      "grad_norm": 0.9181971549987793,
      "learning_rate": 5.2459440456933154e-05,
      "loss": 1.6098,
      "step": 3280
    },
    {
      "epoch": 0.6690997566909975,
      "grad_norm": 0.8483784198760986,
      "learning_rate": 5.13195064411542e-05,
      "loss": 1.6071,
      "step": 3300
    },
    {
      "epoch": 0.6731549067315491,
      "grad_norm": 0.9020887613296509,
      "learning_rate": 5.018780346588019e-05,
      "loss": 1.5895,
      "step": 3320
    },
    {
      "epoch": 0.6772100567721006,
      "grad_norm": 0.8527446985244751,
      "learning_rate": 4.9064522882766595e-05,
      "loss": 1.6093,
      "step": 3340
    },
    {
      "epoch": 0.681265206812652,
      "grad_norm": 0.6858400106430054,
      "learning_rate": 4.794985461938608e-05,
      "loss": 1.5862,
      "step": 3360
    },
    {
      "epoch": 0.6853203568532036,
      "grad_norm": 0.8640597462654114,
      "learning_rate": 4.6843987147115066e-05,
      "loss": 1.6096,
      "step": 3380
    },
    {
      "epoch": 0.689375506893755,
      "grad_norm": 0.8453356027603149,
      "learning_rate": 4.5747107449266425e-05,
      "loss": 1.6173,
      "step": 3400
    },
    {
      "epoch": 0.6934306569343066,
      "grad_norm": 0.7929614782333374,
      "learning_rate": 4.465940098947374e-05,
      "loss": 1.7227,
      "step": 3420
    },
    {
      "epoch": 0.6974858069748581,
      "grad_norm": 0.9674545526504517,
      "learning_rate": 4.3581051680332683e-05,
      "loss": 1.6721,
      "step": 3440
    },
    {
      "epoch": 0.7015409570154095,
      "grad_norm": 0.8458256125450134,
      "learning_rate": 4.25122418523045e-05,
      "loss": 1.6492,
      "step": 3460
    },
    {
      "epoch": 0.7055961070559611,
      "grad_norm": 0.8619134426116943,
      "learning_rate": 4.14531522228869e-05,
      "loss": 1.6054,
      "step": 3480
    },
    {
      "epoch": 0.7096512570965126,
      "grad_norm": 0.8316335678100586,
      "learning_rate": 4.040396186605803e-05,
      "loss": 1.6601,
      "step": 3500
    },
    {
      "epoch": 0.7137064071370641,
      "grad_norm": 0.8089392185211182,
      "learning_rate": 3.9364848181998e-05,
      "loss": 1.6092,
      "step": 3520
    },
    {
      "epoch": 0.7177615571776156,
      "grad_norm": 0.8650294542312622,
      "learning_rate": 3.833598686709351e-05,
      "loss": 1.7048,
      "step": 3540
    },
    {
      "epoch": 0.721816707218167,
      "grad_norm": 0.7797386050224304,
      "learning_rate": 3.731755188423069e-05,
      "loss": 1.6561,
      "step": 3560
    },
    {
      "epoch": 0.7258718572587186,
      "grad_norm": 0.7587558627128601,
      "learning_rate": 3.630971543338092e-05,
      "loss": 1.637,
      "step": 3580
    },
    {
      "epoch": 0.7299270072992701,
      "grad_norm": 0.8246535658836365,
      "learning_rate": 3.531264792248462e-05,
      "loss": 1.5812,
      "step": 3600
    },
    {
      "epoch": 0.7339821573398215,
      "grad_norm": 0.9051045775413513,
      "learning_rate": 3.432651793863838e-05,
      "loss": 1.6255,
      "step": 3620
    },
    {
      "epoch": 0.7380373073803731,
      "grad_norm": 0.8805311322212219,
      "learning_rate": 3.33514922195897e-05,
      "loss": 1.5977,
      "step": 3640
    },
    {
      "epoch": 0.7420924574209246,
      "grad_norm": 0.8887985348701477,
      "learning_rate": 3.2387735625544244e-05,
      "loss": 1.6191,
      "step": 3660
    },
    {
      "epoch": 0.7461476074614761,
      "grad_norm": 0.9139361381530762,
      "learning_rate": 3.1435411111291304e-05,
      "loss": 1.6031,
      "step": 3680
    },
    {
      "epoch": 0.7502027575020276,
      "grad_norm": 0.6351070404052734,
      "learning_rate": 3.0494679698650353e-05,
      "loss": 1.6593,
      "step": 3700
    },
    {
      "epoch": 0.754257907542579,
      "grad_norm": 0.7965565919876099,
      "learning_rate": 2.9565700449245405e-05,
      "loss": 1.5975,
      "step": 3720
    },
    {
      "epoch": 0.7583130575831306,
      "grad_norm": 0.8785256743431091,
      "learning_rate": 2.864863043761026e-05,
      "loss": 1.6322,
      "step": 3740
    },
    {
      "epoch": 0.7623682076236821,
      "grad_norm": 0.8347001075744629,
      "learning_rate": 2.7743624724629848e-05,
      "loss": 1.598,
      "step": 3760
    },
    {
      "epoch": 0.7664233576642335,
      "grad_norm": 0.8123968243598938,
      "learning_rate": 2.685083633132216e-05,
      "loss": 1.6202,
      "step": 3780
    },
    {
      "epoch": 0.7704785077047851,
      "grad_norm": 0.7974863052368164,
      "learning_rate": 2.5970416212965044e-05,
      "loss": 1.6635,
      "step": 3800
    },
    {
      "epoch": 0.7745336577453366,
      "grad_norm": 0.8847312927246094,
      "learning_rate": 2.5102513233572013e-05,
      "loss": 1.6893,
      "step": 3820
    },
    {
      "epoch": 0.7785888077858881,
      "grad_norm": 0.9320250153541565,
      "learning_rate": 2.4247274140722198e-05,
      "loss": 1.644,
      "step": 3840
    },
    {
      "epoch": 0.7826439578264396,
      "grad_norm": 0.895386278629303,
      "learning_rate": 2.3404843540747634e-05,
      "loss": 1.6258,
      "step": 3860
    },
    {
      "epoch": 0.786699107866991,
      "grad_norm": 0.7840206027030945,
      "learning_rate": 2.2575363874282786e-05,
      "loss": 1.5629,
      "step": 3880
    },
    {
      "epoch": 0.7907542579075426,
      "grad_norm": 0.8032273650169373,
      "learning_rate": 2.1758975392180403e-05,
      "loss": 1.586,
      "step": 3900
    }
  ],
  "logging_steps": 20,
  "max_steps": 4932,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.879164465230643e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
