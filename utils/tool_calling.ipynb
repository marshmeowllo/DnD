{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ea2803",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29071199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuaylong/miniconda3/envs/nlp/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: '__init_subclass__' (from 'transformers.agents.tools') is deprecated and will be removed from version '4.51.0'. Switch to smolagents instead, with the same functionalities and similar API (https://huggingface.co/docs/smolagents/index)\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import uuid\n",
    "import datetime\n",
    "\n",
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "from langchain.tools import tool\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, TransformersEngine, CodeAgent, BitsAndBytesConfig\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "\n",
    "from lightning import Fabric\n",
    "from peft import LoraConfig, get_peft_model, PeftModelForCausalLM, PeftModel\n",
    "\n",
    "from IPython.display import display, Markdown, Image, SVG\n",
    "from os import walk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd806861",
   "metadata": {},
   "source": [
    "### Set mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2463854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "fabric = Fabric(accelerator=\"cuda\", devices=1, precision=\"bf16-mixed\")\n",
    "device = fabric.device\n",
    "fabric.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b92fb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 27 15:27:38 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080        Off |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   54C    P5             27W /  340W |    9589MiB /  16376MiB |     57%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2775      G   /usr/lib/xorg/Xorg                     1180MiB |\n",
      "|    0   N/A  N/A            3067      G   /usr/bin/gnome-shell                    283MiB |\n",
      "|    0   N/A  N/A            3769      G   /opt/microsoft/msedge/msedge              3MiB |\n",
      "|    0   N/A  N/A            3823      G   ...144 --variations-seed-version        279MiB |\n",
      "|    0   N/A  N/A            8223      G   /usr/bin/nautilus                        23MiB |\n",
      "|    0   N/A  N/A           42266      G   ...ess --variations-seed-version       1053MiB |\n",
      "|    0   N/A  N/A           45522      C   ...iniconda3/envs/nlp/bin/python       6570MiB |\n",
      "|    0   N/A  N/A           46072      G   ...and --variations-seed-version         58MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5e83247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f955ad8ff9b74b19a6829556b28986b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"Salesforce/Llama-xLAM-2-8b-fc-r\"\n",
    "# model_name = \"NousResearch/Hermes-3-Llama-3.1-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea700ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c20238b4",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/how_to/tools_prompting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd857be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "get_weather\n",
      "Get the current weather for a city.\n",
      "Args:\n",
      "    city (str): The name of the city.\n",
      "\n",
      "Returns:\n",
      "    str: The current weather in the city.\n",
      "{'city': {'title': 'City', 'type': 'string'}}\n",
      "--\n",
      "add\n",
      "Add two numbers.\n",
      "{'x': {'title': 'X', 'type': 'integer'}, 'y': {'title': 'Y', 'type': 'integer'}}\n",
      "--\n",
      "multiply\n",
      "Multiply two numbers together.\n",
      "{'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the current weather for a city.\n",
    "    Args:\n",
    "        city (str): The name of the city.\n",
    "        \n",
    "    Returns:\n",
    "        str: The current weather in the city.\n",
    "    \"\"\"\n",
    "    return f\"The weather in {city} is sunny.\"\n",
    "\n",
    "@tool\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"Add two numbers.\"\n",
    "    return x + y\n",
    "\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply two numbers together.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "tools = [\n",
    "    get_weather,\n",
    "    add,\n",
    "    multiply,\n",
    "]\n",
    "\n",
    "for t in tools:\n",
    "    print(\"--\")\n",
    "    print(t.name)\n",
    "    print(t.description)\n",
    "    print(t.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3547895e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_weather(city: str) -> str - Get the current weather for a city.\n",
      "Args:\n",
      "    city (str): The name of the city.\n",
      "\n",
      "Returns:\n",
      "    str: The current weather in the city.\n",
      "add(x: int, y: int) -> int - Add two numbers.\n",
      "multiply(x: float, y: float) -> float - Multiply two numbers together.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import render_text_description\n",
    "\n",
    "rendered_tools = render_text_description(tools)\n",
    "print(rendered_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35db1d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=2048,\n",
    "    top_k=10,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af54df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessage(f\"\"\"\\\n",
    "You are an assistant that has access to the following set of tools. \n",
    "Here are the names and descriptions for each tool:\n",
    "\n",
    "{rendered_tools}\n",
    "\n",
    "Given the user input, return the name and input of the tool to use. \n",
    "Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
    "\n",
    "The `arguments` should be a dictionary, with keys corresponding \n",
    "to the argument names and the values corresponding to the requested values.\n",
    "\"\"\")\n",
    "\n",
    "chat = ChatHuggingFace(llm=llm, tokenizer=tokenizer).bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8240db3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"add\", \"arguments\": {\"x\": 3, \"y\": 1132}}\n"
     ]
    }
   ],
   "source": [
    "query = \"what's 3 plus 1132?\"\n",
    "\n",
    "messages = [system_prompt, HumanMessage(query)]\n",
    "\n",
    "message = chat.invoke(messages)\n",
    "\n",
    "if isinstance(message, str):\n",
    "    print(message)\n",
    "else:  # Otherwise it's a chat model\n",
    "    print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e64ad98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'add', 'arguments': {'x': 3, 'y': 1132}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "chain = chat | JsonOutputParser()\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73140299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional, TypedDict\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "class ToolCallRequest(TypedDict):\n",
    "    \"\"\"A typed dict that shows the inputs into the invoke_tool function.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    arguments: Dict[str, Any]\n",
    "\n",
    "\n",
    "def invoke_tool(\n",
    "    tool_call_request: ToolCallRequest, config: Optional[RunnableConfig] = None\n",
    "):\n",
    "    \"\"\"A function that we can use the perform a tool invocation.\n",
    "\n",
    "    Args:\n",
    "        tool_call_request: a dict that contains the keys name and arguments.\n",
    "            The name must match the name of a tool that exists.\n",
    "            The arguments are the arguments to that tool.\n",
    "        config: This is configuration information that LangChain uses that contains\n",
    "            things like callbacks, metadata, etc.See LCEL documentation about RunnableConfig.\n",
    "\n",
    "    Returns:\n",
    "        output from the requested tool\n",
    "    \"\"\"\n",
    "    print(\"Tool call request:\", tool_call_request)\n",
    "    \n",
    "    if isinstance(tool_call_request, list):\n",
    "        tool_call_request = tool_call_request[-1]\n",
    "        \n",
    "    tool_name_to_tool = {tool.name: tool for tool in tools}\n",
    "    name = tool_call_request[\"name\"]\n",
    "    requested_tool = tool_name_to_tool[name]\n",
    "    return requested_tool.invoke(tool_call_request[\"arguments\"], config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d0f1bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool call request: [{'name': 'get_weather', 'arguments': {'city': 'San Francisco'}}]\n",
      "The weather in San Francisco is sunny.\n"
     ]
    }
   ],
   "source": [
    "chain = chat | JsonOutputParser() | invoke_tool\n",
    "\n",
    "query = \"what's the weather in San Francisco?\"\n",
    "\n",
    "messages = [system_prompt, HumanMessage(query)]\n",
    "\n",
    "response = chain.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f61bbd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
