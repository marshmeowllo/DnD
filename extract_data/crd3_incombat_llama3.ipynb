{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6444343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sonso\\Docs\\CEDT\\2_2\\nlp_sys\\nlp\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import gc\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from lightning import Fabric\n",
    "\n",
    "from huggingface_hub import login\n",
    "from transformers import get_scheduler, pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModelForCausalLM\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "# import warnings\n",
    "# warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "%config InlineBackend.figure_formats = ['svg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fffc4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799c4d90a8d34f269e0fec7fe9eefbe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sonso\\Docs\\CEDT\\2_2\\nlp_sys\\nlp\\Lib\\site-packages\\lightning\\fabric\\connector.py:571: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "c:\\Users\\sonso\\Docs\\CEDT\\2_2\\nlp_sys\\nlp\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "c:\\Users\\sonso\\Docs\\CEDT\\2_2\\nlp_sys\\nlp\\Lib\\site-packages\\peft\\peft_model.py:569: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.28.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.28.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.29.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.29.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.30.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.30.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.31.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.31.self_attn.k_proj.lora_magnitude_vector.default.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "student_model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    use_dora=True,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\"],\n",
    "    lora_dropout=0.2,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "student_model = AutoModelForCausalLM.from_pretrained(student_model_name, device_map=\"auto\", quantization_config=quant_config,)\n",
    "tokenizer = AutoTokenizer.from_pretrained(student_model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "for param in student_model.parameters():\n",
    "  param.requires_grad = False\n",
    "\n",
    "student_model.gradient_checkpointing_enable()\n",
    "student_model.enable_input_require_grads()\n",
    "\n",
    "\n",
    "lora_model = get_peft_model(student_model, lora_config)\n",
    "\n",
    "fabric = Fabric(accelerator=\"cuda\", devices=1, precision=\"bf16\")\n",
    "device = fabric.device\n",
    "fabric.launch()\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "model = PeftModelForCausalLM.from_pretrained(lora_model, \"./incombat_best\")\n",
    "\n",
    "def generate_response_with_role(model=model, user_input=[], max_length=4096):\n",
    "    prompt = [\n",
    "      {\"role\": \"system\", \"content\": \"in a text-based adventure (Dungeons and Dragons).\\n\\nYour job is to analyze conversation dialog and answer if the players are in a combat state or not, providee your reasoning , and give final answer as 'IN_COMBAT' or 'NOT_IN_COMBAT' only.\\n\\n Here is the dialogs:\"},\n",
    "    ]\n",
    "\n",
    "    prompt.extend(user_input)\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            eos_token_id=terminators,\n",
    "            num_return_sequences=1,\n",
    "            temperature=1.0,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76524f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': '<DM>', 'content': '\"That\\'s okay. Just a second. You let us know when it\\'s okay to talk.\" [character: The Dungeon Master, class: -, race: -, in_context: False, items: -]'}, {'role': '<PLAYER1>', 'content': '\"I don\\'t know what that means, a bottleneck.\" [character: Grog Strongjaw, class: Barbarian (Path of the Berserker)/Fighter (Battle Master), race: Goliath, in_context: False, items: -]'}, {'role': '<DM>', 'content': '\"Are we all right? And how are we? Exterminate. Exterminate.\" [character: The Dungeon Master, class: -, race: -, in_context: False, items: -]'}, {'role': '<PLAYER4>', 'content': '\"Nice. Audio isn\\'t working they\\'re going to watch something else. Thanks, douche. (laughter)\" [character: Scanlan Shorthalt, class: Bard (College of Lore), race: Gnome, in_context: False, items: -]'}, {'role': '<DM>', 'content': '\"Is the mic adjusted? Is it any better?\" [character: The Dungeon Master, class: -, race: -, in_context: False, items: -]'}, {'role': '<PLAYER8>', 'content': '\"Better now. We\\'re good.\" [character: -, class: -, race: -, in_context: False, items: -]'}, {'role': '<DM>', 'content': '\"Well, great. Thank you guys. A couple of things transitioned from Pathfinder to 5th edition may be a little strange and fun. We have a lot of players, but they\\'re great folks and most of that just means I have to work really hard to make sure the game keeps going, so that\\'s on me. Ashley Johnson, unfortunately, cannot join us. She\\'s our gnome cleric, Pike. She\\'ll be here next week. Unfortunately, she\\'s in London accepting a BAFTA, so I think we\\'re okay. For the Last of Us. That\\'s harsh. Also no donations this week, but going forward we will have donations available for those that will help partially run the show from our wonderful cameraman and crew. Also, half of our donations will go to the 826 charity. Marisha, if you want to talk a little about that.\" [character: The Dungeon Master, class: -, race: -, in_context: False, items: -]'}, {'role': '<PLAYER2>', 'content': '\"Yes! 826LA is a charity that focuses on tutoring children in an after-school program and helping specifically with creativity, storytelling, art and general things that relate to Dungeons & Dragons. You should definitely check out the Time Travel Mart. There is one in Echo Park and one in Mar Vista. They have this amazing storefront where you can buy these awesome tchotchkes and things that go immediately to support the charity and the stream. It\\'s awesome. It\\'s 826LA, you should look it up. There\\'s a few all over the country.\" [character: Keyleth, class: Druid (Circle of the Moon), race: Half-elf, in_context: False, items: -]'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load from file\n",
    "with open(\"extracted_to_llama_v1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    messages = json.load(f)\n",
    "\n",
    "print(messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eea0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# message = [\n",
    "#     {\"role\": \"<PLAYER1>\", \"content\": \"the goblin is attacking me! can i run away?\"},\n",
    "#     {\"role\": \"<DM>\", \"content\": \"no, you have a slowness debuff, you can't run away.\"},\n",
    "#     {\"role\": \"<PLAYER2>\", \"content\": \"i will cast fireball on the goblin.\"},\n",
    "# ]\n",
    "prompt = [\n",
    "      {\"role\": \"system\", \"content\": \"in a text-based adventure (Dungeons and Dragons).\\n\\nYour job is to analyze conversation dialog and answer if the players are in a combat state or not, providee your reasoning , and give final answer as 'IN_COMBAT' or 'NOT_IN_COMBAT' only.\\n\\n Here is the dialogs:\"},\n",
    "    ]\n",
    "\n",
    "prompt.extend(messages)\n",
    "\n",
    "# prompt\n",
    "# tokenizer.apply_chat_template(prompt, tokenize=False)\n",
    "# input_ids = tokenizer.apply_chat_template(\n",
    "#         prompt,\n",
    "#         add_generation_prompt=True,\n",
    "#         return_tensors=\"pt\",\n",
    "#         # tokenize=False\n",
    "#     ).to(model.device)\n",
    "response = generate_response_with_role(user_input=messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f342f369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Based on the conversation, I analyze the context and conclude that:\\n'\n",
      " '\\n'\n",
      " 'The players are NOT IN_COMBAT.\\n'\n",
      " '\\n'\n",
      " 'Reasoning:\\n'\n",
      " '\\n'\n",
      " '* The conversation is not about combat, fighting, or any actions that would '\n",
      " 'indicate a battle is taking place.\\n'\n",
      " '* The players are discussing technical issues with audio, adjusting '\n",
      " 'settings, and making arrangements for future donations and charity support.\\n'\n",
      " '* The Dungeon Master is not describing a combat scenario or providing '\n",
      " 'combat-related information.\\n'\n",
      " '* The players are not engaging in combat-related role-playing or actions.\\n'\n",
      " '* The conversation is focused on administrative and organizational aspects '\n",
      " 'of the game, such as technical issues and charity support.\\n'\n",
      " '\\n'\n",
      " 'Therefore, my answer is: NOT_IN_COMBAT.')\n"
     ]
    }
   ],
   "source": [
    "pprint(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
