{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ea2803",
   "metadata": {},
   "source": [
    "# Chat With Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29071199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
<<<<<<< HEAD
    "from peft import PeftModel, LoraConfig\n",
=======
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings, ChatHuggingFace\n",
    "from langchain.tools import tool\n",
    "\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "from langchain_core.messages import SystemMessage, AnyMessage\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.runnables import RunnableConfig, Runnable\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from typing import Annotated, Any, Dict, Optional, TypedDict, Union, List\n",
    "from lightning import Fabric\n",
    "\n",
    "from IPython.display import display, Markdown, Image, SVG\n",
    "from os import walk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd806861",
   "metadata": {},
   "source": [
    "### Set mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2463854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "fabric = Fabric(accelerator=\"cuda\", devices=1, precision=\"bf16-mixed\")\n",
    "device = fabric.device\n",
    "fabric.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc9f613",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52614d3",
   "metadata": {},
   "source": [
    "### Load Embedding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51f3676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embed_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9100489",
   "metadata": {},
   "source": [
    "### Load vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f30b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.load_local(\n",
<<<<<<< HEAD
    "    \"./faiss_dnd_index\",\n",
=======
    "    \"./faiss_spell_index\",\n",
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
    "    embeddings=embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf13e68",
   "metadata": {},
   "source": [
    "### Mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1000e2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>email</th>\n",
       "      <th>num_posts</th>\n",
       "      <th>num_likes</th>\n",
       "      <th>num_followers</th>\n",
       "      <th>num_following</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_1</td>\n",
       "      <td>User1</td>\n",
<<<<<<< HEAD
       "      <td>30</td>\n",
       "      <td>user1@example.com</td>\n",
       "      <td>311</td>\n",
       "      <td>7777</td>\n",
       "      <td>37874</td>\n",
       "      <td>202</td>\n",
=======
       "      <td>41</td>\n",
       "      <td>user1@example.com</td>\n",
       "      <td>175</td>\n",
       "      <td>4880</td>\n",
       "      <td>33335</td>\n",
       "      <td>3198</td>\n",
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_2</td>\n",
       "      <td>User2</td>\n",
<<<<<<< HEAD
       "      <td>57</td>\n",
       "      <td>user2@example.com</td>\n",
       "      <td>380</td>\n",
       "      <td>4590</td>\n",
       "      <td>56551</td>\n",
       "      <td>3237</td>\n",
=======
       "      <td>58</td>\n",
       "      <td>user2@example.com</td>\n",
       "      <td>34</td>\n",
       "      <td>8090</td>\n",
       "      <td>48685</td>\n",
       "      <td>988</td>\n",
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_3</td>\n",
       "      <td>User3</td>\n",
<<<<<<< HEAD
       "      <td>57</td>\n",
       "      <td>user3@example.com</td>\n",
       "      <td>46</td>\n",
       "      <td>8255</td>\n",
       "      <td>1651</td>\n",
       "      <td>2249</td>\n",
=======
       "      <td>50</td>\n",
       "      <td>user3@example.com</td>\n",
       "      <td>13</td>\n",
       "      <td>1779</td>\n",
       "      <td>51522</td>\n",
       "      <td>4200</td>\n",
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_4</td>\n",
       "      <td>User4</td>\n",
<<<<<<< HEAD
       "      <td>50</td>\n",
       "      <td>user4@example.com</td>\n",
       "      <td>122</td>\n",
       "      <td>3045</td>\n",
       "      <td>57246</td>\n",
       "      <td>3679</td>\n",
=======
       "      <td>32</td>\n",
       "      <td>user4@example.com</td>\n",
       "      <td>497</td>\n",
       "      <td>6144</td>\n",
       "      <td>45073</td>\n",
       "      <td>3324</td>\n",
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_5</td>\n",
       "      <td>User5</td>\n",
<<<<<<< HEAD
       "      <td>25</td>\n",
       "      <td>user5@example.com</td>\n",
       "      <td>421</td>\n",
       "      <td>1078</td>\n",
       "      <td>85481</td>\n",
       "      <td>3883</td>\n",
=======
       "      <td>55</td>\n",
       "      <td>user5@example.com</td>\n",
       "      <td>11</td>\n",
       "      <td>6269</td>\n",
       "      <td>92852</td>\n",
       "      <td>4376</td>\n",
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>user_6</td>\n",
       "      <td>User6</td>\n",
<<<<<<< HEAD
       "      <td>32</td>\n",
       "      <td>user6@example.com</td>\n",
       "      <td>470</td>\n",
       "      <td>7942</td>\n",
       "      <td>97368</td>\n",
       "      <td>2791</td>\n",
=======
       "      <td>50</td>\n",
       "      <td>user6@example.com</td>\n",
       "      <td>160</td>\n",
       "      <td>5745</td>\n",
       "      <td>36162</td>\n",
       "      <td>3986</td>\n",
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>user_7</td>\n",
       "      <td>User7</td>\n",
<<<<<<< HEAD
       "      <td>47</td>\n",
       "      <td>user7@example.com</td>\n",
       "      <td>455</td>\n",
       "      <td>96</td>\n",
       "      <td>17072</td>\n",
       "      <td>1573</td>\n",
=======
       "      <td>42</td>\n",
       "      <td>user7@example.com</td>\n",
       "      <td>68</td>\n",
       "      <td>4740</td>\n",
       "      <td>39888</td>\n",
       "      <td>3039</td>\n",
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>user_8</td>\n",
       "      <td>User8</td>\n",
<<<<<<< HEAD
       "      <td>48</td>\n",
       "      <td>user8@example.com</td>\n",
       "      <td>2</td>\n",
       "      <td>5761</td>\n",
       "      <td>41789</td>\n",
       "      <td>3019</td>\n",
=======
       "      <td>57</td>\n",
       "      <td>user8@example.com</td>\n",
       "      <td>58</td>\n",
       "      <td>7048</td>\n",
       "      <td>75968</td>\n",
       "      <td>1964</td>\n",
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>user_9</td>\n",
       "      <td>User9</td>\n",
<<<<<<< HEAD
       "      <td>18</td>\n",
       "      <td>user9@example.com</td>\n",
       "      <td>228</td>\n",
       "      <td>1958</td>\n",
       "      <td>33844</td>\n",
       "      <td>4538</td>\n",
=======
       "      <td>54</td>\n",
       "      <td>user9@example.com</td>\n",
       "      <td>125</td>\n",
       "      <td>1783</td>\n",
       "      <td>63756</td>\n",
       "      <td>1916</td>\n",
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>user_10</td>\n",
       "      <td>User10</td>\n",
<<<<<<< HEAD
       "      <td>53</td>\n",
       "      <td>user10@example.com</td>\n",
       "      <td>155</td>\n",
       "      <td>9690</td>\n",
       "      <td>23873</td>\n",
       "      <td>3086</td>\n",
=======
       "      <td>37</td>\n",
       "      <td>user10@example.com</td>\n",
       "      <td>239</td>\n",
       "      <td>2566</td>\n",
       "      <td>10662</td>\n",
       "      <td>3900</td>\n",
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id    name  age               email  num_posts  num_likes  \\\n",
<<<<<<< HEAD
       "0   user_1   User1   30   user1@example.com        311       7777   \n",
       "1   user_2   User2   57   user2@example.com        380       4590   \n",
       "2   user_3   User3   57   user3@example.com         46       8255   \n",
       "3   user_4   User4   50   user4@example.com        122       3045   \n",
       "4   user_5   User5   25   user5@example.com        421       1078   \n",
       "5   user_6   User6   32   user6@example.com        470       7942   \n",
       "6   user_7   User7   47   user7@example.com        455         96   \n",
       "7   user_8   User8   48   user8@example.com          2       5761   \n",
       "8   user_9   User9   18   user9@example.com        228       1958   \n",
       "9  user_10  User10   53  user10@example.com        155       9690   \n",
       "\n",
       "   num_followers  num_following  \n",
       "0          37874            202  \n",
       "1          56551           3237  \n",
       "2           1651           2249  \n",
       "3          57246           3679  \n",
       "4          85481           3883  \n",
       "5          97368           2791  \n",
       "6          17072           1573  \n",
       "7          41789           3019  \n",
       "8          33844           4538  \n",
       "9          23873           3086  "
=======
       "0   user_1   User1   41   user1@example.com        175       4880   \n",
       "1   user_2   User2   58   user2@example.com         34       8090   \n",
       "2   user_3   User3   50   user3@example.com         13       1779   \n",
       "3   user_4   User4   32   user4@example.com        497       6144   \n",
       "4   user_5   User5   55   user5@example.com         11       6269   \n",
       "5   user_6   User6   50   user6@example.com        160       5745   \n",
       "6   user_7   User7   42   user7@example.com         68       4740   \n",
       "7   user_8   User8   57   user8@example.com         58       7048   \n",
       "8   user_9   User9   54   user9@example.com        125       1783   \n",
       "9  user_10  User10   37  user10@example.com        239       2566   \n",
       "\n",
       "   num_followers  num_following  \n",
       "0          33335           3198  \n",
       "1          48685            988  \n",
       "2          51522           4200  \n",
       "3          45073           3324  \n",
       "4          92852           4376  \n",
       "5          36162           3986  \n",
       "6          39888           3039  \n",
       "7          75968           1964  \n",
       "8          63756           1916  \n",
       "9          10662           3900  "
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_users = 10\n",
    "\n",
    "data = {\n",
    "    \"user_id\": [f\"user_{i+1}\" for i in range(n_users)],\n",
    "    \"name\": [f\"User{i+1}\" for i in range(n_users)],\n",
    "    \"age\": np.random.randint(18, 60, size=n_users),\n",
    "    \"email\": [f\"user{i+1}@example.com\" for i in range(n_users)],\n",
    "    \"num_posts\": np.random.randint(0, 500, size=n_users),\n",
    "    \"num_likes\": np.random.randint(0, 10000, size=n_users),\n",
    "    \"num_followers\": np.random.randint(0, 100000, size=n_users),\n",
    "    \"num_following\": np.random.randint(0, 5000, size=n_users),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2efde47",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd857be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def spell_retrieve(query: str) -> str:\n",
    "    \"\"\"Retrieve information about dungeons and dragons spell.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The spell name to search for.\n",
    "\n",
    "    Returns:\n",
    "        str: The spell information.\n",
    "    \"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=3)\n",
    "\n",
    "    contents = \"\\n\\n\".join(\n",
    "        (f\"{doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    \n",
    "    return contents\n",
    "\n",
    "@tool\n",
    "def user(name: str) -> str:\n",
    "    \"\"\"\n",
    "    User infomation retreiver\n",
    "\n",
    "    Args:\n",
    "        name (str): The name of user.\n",
    "\n",
    "    Returns:\n",
    "        str: The user information.\n",
    "    \"\"\"\n",
    "    return f'{df[df['name'] == name]}.'\n",
    "\n",
    "tools = [\n",
    "    spell_retrieve,\n",
    "    user\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb85cf",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "```\n",
    "[User Message] \n",
    "     ↓\n",
    "[xLAM-2] → [Check if Tool Call is Required] \n",
    "     ├── No → [Return to Llama 3 with \"No tool needed\"]\n",
    "     └── Yes → [Detect Tool to Call] → [Prepare Structured Tool Call] → [Execute Tool] → [Send Result to Llama 3]\n",
    "                          ↓\n",
    "               [Llama 3] → [Generate Final Natural Reply]\n",
    "     ↓\n",
    "[Final reply to User]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e4861",
   "metadata": {},
   "source": [
    "## Custom Sate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53a248c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    name: str\n",
    "    stat: str\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    context: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905fe375",
   "metadata": {},
   "source": [
    "## Tool Calling Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af54df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolCallRequest(TypedDict):\n",
    "    \"\"\"A typed dict that shows the inputs into the invoke_tool function.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    arguments: Dict[str, Any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "909ac8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolCalling():\n",
    "    def __init__(self, model_name: str, tools: list[BaseTool]):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"cpu\")\n",
    "        self.pipe = pipeline(\n",
    "            task=\"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            return_full_text=False,\n",
    "            max_new_tokens=512,\n",
    "            top_k=10,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self._llm = HuggingFacePipeline(pipeline=self.pipe)\n",
    "        self.chat = ChatHuggingFace(llm=self._llm, tokenizer=self.tokenizer)\n",
    "        self.rendered_tools = [convert_to_openai_tool(f) for f in tools]\n",
    "\n",
    "    def invoke_tool(\n",
    "            self,\n",
    "            tool_call_request: Union[ToolCallRequest, List[ToolCallRequest]], \n",
    "            config: Optional[RunnableConfig] = None\n",
    "    ):\n",
    "        \"\"\"A function that we can use the perform a tool invocation.\n",
    "\n",
    "        Args:\n",
    "            tool_call_request: a dict that contains the keys name and arguments.\n",
    "                The name must match the name of a tool that exists.\n",
    "                The arguments are the arguments to that tool.\n",
    "            config: This is configuration information that LangChain uses that contains\n",
    "                things like callbacks, metadata, etc.See LCEL documentation about RunnableConfig.\n",
    "\n",
    "        Returns:\n",
    "            output from the requested tool\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Tool call request:\", tool_call_request)\n",
    "        \n",
    "        # Sometimes the model outputs a list of tool call requests, \n",
    "        # so I loop each tool call and append to list\n",
    "        \n",
    "        if isinstance(tool_call_request, list):\n",
    "            output = list()\n",
    "\n",
    "            for tool_call in tool_call_request:\n",
    "                tool_name_to_tool = {tool.name: tool for tool in tools}\n",
    "                name = tool_call[\"name\"]\n",
    "                requested_tool = tool_name_to_tool[name]\n",
    "                output.append(requested_tool.invoke(tool_call[\"arguments\"], config=config))\n",
    "            return output\n",
    "        \n",
    "        tool_name_to_tool = {tool.name: tool for tool in tools}\n",
    "        name = tool_call_request[\"name\"]\n",
    "        requested_tool = tool_name_to_tool[name]\n",
    "        return requested_tool.invoke(tool_call_request[\"arguments\"], config=config)\n",
    "    \n",
    "    def invoke(self, state: State) -> List[str]:\n",
    "        system_prompt = SystemMessage(f\"\"\"\\\n",
    "        You are an assistant that has access to the following set of tools. \n",
    "        Here are the names and descriptions for each tool:\n",
    "\n",
    "        {self.rendered_tools}\n",
    "\n",
    "        Given the user input, return the name and input of the tool to use. \n",
    "        Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
    "\n",
    "        The `arguments` should be a dictionary, with keys corresponding \n",
    "        to the argument names and the values corresponding to the requested values.\n",
    "        \"\"\")\n",
    "\n",
    "        chain = self.chat | JsonOutputParser() | self.invoke_tool\n",
    "        messages = [system_prompt] + [state['messages'][-1]]\n",
    "\n",
    "        try:\n",
    "            response = chain.invoke(messages)\n",
    "        except Exception as e:\n",
    "            print(\"Error invoking tool:\", e)\n",
    "            response = [\"No tool needed\"]\n",
    "\n",
    "        if not response:\n",
    "            response = [\"No tool needed\"]\n",
    "        \n",
    "        return {\"context\": AIMessage(response)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ece929",
   "metadata": {},
   "source": [
    "### ToolCalling Class Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83203c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "715b9ed820614469a790915cda89b3f6",
=======
       "model_id": "38980ca224674ac9a95c6116cd33c784",
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "tool_calling_model_name = \"Salesforce/Llama-xLAM-2-8b-fc-r\"\n",
    "\n",
    "tool_calling = ToolCalling(\n",
    "    model_name=tool_calling_model_name,\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a07f6b1c",
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool call request: [{'name': 'user', 'arguments': {'name': 'player_3'}}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'player_3',\n",
       " 'messages': [HumanMessage(content='<|start_header_id|>player_3<|end_header_id|>\\nWhere am I?<|eot_id|>', additional_kwargs={}, response_metadata={}, id='2daec63b-1d65-46e4-9105-da7f3170579a')],\n",
       " 'context': AIMessage(content=['Empty DataFrame\\nColumns: [user_id, name, age, email, num_posts, num_likes, num_followers, num_following]\\nIndex: [].'], additional_kwargs={}, response_metadata={})}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
   "source": [
    "def tool(state: State) -> str:\n",
    "    return tool_calling.invoke(state)\n",
    "\n",
    "graph = StateGraph(State)\n",
    "graph.add_edge(START, \"tool call\")\n",
    "graph.add_node(\"tool call\", tool)\n",
    "graph.add_edge(\"tool call\", END)\n",
    "\n",
<<<<<<< HEAD
    "graph = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b53bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\"],\n",
    "    lora_dropout=0.2,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
=======
    "graph = graph.compile()\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "input_message = {\n",
    "    \"name\": \"player_3\",\n",
    "    \"messages\": [HumanMessage(content=\"<|start_header_id|>player_3<|end_header_id|>\\nWhere am I?<|eot_id|>\")],\n",
    "    \"context\": \"\"\n",
    "}\n",
    "\n",
    "graph.invoke(input_message)"
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60520439",
   "metadata": {},
   "source": [
    "## Llama-3 Chat Class"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 12,
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
   "id": "5a0b8be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaChat():\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, load_in_4bit=True)\n",
<<<<<<< HEAD
    "        self.adapter_model_name = './weights/bestRL_SFT'\n",
    "        self.model = PeftModel.from_pretrained(self.model, self.adapter_model_name)\n",
    "        self.model = self.model.merge_and_unload()\n",
=======
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
    "        self.pipe = pipeline(\n",
    "            task=\"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            return_full_text=False,\n",
    "            max_new_tokens=2048,\n",
    "            top_k=10,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self._llm = HuggingFacePipeline(pipeline=self.pipe)\n",
    "        self.chat = ChatHuggingFace(llm=self._llm, tokenizer=self.tokenizer)\n",
    "\n",
    "    def generate(self, state: State) -> Dict[str, Any]:\n",
    "        system_message_content = (\n",
<<<<<<< HEAD
    "            \"<|begin_of_text|>\\n\"\n",
    "            \"<|start_header_id|>system<|end_header_id|>\\n\"\n",
    "            \"You are a highly skilled Dungeon Master (DM) for Dungeons & Dragons 5th Edition. \"\n",
    "            \"Your job is to read the player’s input and reply with an immersive, clear, and engaging narration that advances the story and game mechanics.\\n\"\n",
    "            \"\\n\"\n",
    "            # \"Advice for the DM:\\n\"\n",
    "            # \"  - Describe scenes with vivid sensory details and atmosphere.\\n\"\n",
    "            # \"  - Offer meaningful choices if players ask for choices; respect player agency and avoid railroading.\\n\"\n",
    "            # \"  - Keep secret die rolls hidden; reveal only the results and their effects.\\n\"\n",
    "            # \"  - Use rules faithfully but prioritize fun and pacing over strict book-keeping.\\n\"\n",
    "            # \"  - Improvise when players surprise you, but maintain internal consistency.\\n\"\n",
    "            # \"  - Avoid technical jargon in your narration—stay in character and tone.\\n\"\n",
    "            # \"\\n\"\n",
    "            \"Basic D&D 5e Rules Summary:\\n\"\n",
    "            \"  - Initiative: roll a d20 + Dexterity modifier to determine turn order.\\n\"\n",
    "            \"  - Actions: on your turn you can take one Action, one Bonus Action (if available), and move up to your speed.\\n\"\n",
    "            \"  - Reactions: special actions triggered outside your turn, such as Opportunity Attacks.\\n\"\n",
    "            \"  - Advantage & Disadvantage: roll two d20s; take the higher roll for advantage or the lower for disadvantage.\\n\"\n",
    "            \"  - Spellcasting: consumes spell slots; cantrips are cast at will; concentration holds one spell at a time.\\n\\n\"\n",
    "            \"Campaign detail:\\n\"\n",
    "            \"At the heart of Barovia, a land shrouded in perpetual mist and the oppressive gloom of Castle Ravenloft looming above, the adventurers find themselves trapped within the cursed domain of Strahd.\\n\"\n",
    "            \"The infamous vampire lord Strahd von Zarovich rules with iron and blood, his tragic past entwined with dark magic and unrequited love.\\n\"\n",
    "            \"The party’s destiny is guided by a cryptic Tarokka deck reading from Madam Eva, randomizing the locations of three powerful artifacts—the Sunsword, the Tome of Strahd, and the Holy Symbol of Ravenkind—to set the stakes for their final confrontation.\\n\"\n",
    "            \"Early survival often hinges on navigating the deathly Svalich Woods, where sentient trees and wendigos prowl, and entering the perilous Death House to learn that not all horrors are undead.\\n\"\n",
    "            \"In the Village of Barovia, the PCs meet Ismark and Ireena Kolyana—siblings haunted by Strahd’s obsession—and may face a night assault in the graveyard by the vampire himself.\\n\"\n",
    "            \"As they pursue their quest, they reach Vallaki’s forced festivals of joy, unmasking Baron Vallakovich’s dark secrets beneath the town’s brittle gaiety. :\\n\"\n",
    "            \"The modular nature of the land allows for side trips to Argynvostholt’s haunted halls, Van Richten’s Tower of mad experimentation, and the haunted Ruins of Berez—each offering unique encounters like the Abbot’s twisted deva or Baba Lysaga’s creeping hut. :\\n\"\n",
    "            \"A crucial detour to the Wizard of Wines winery leads through Yester Hill, where twig blights and corrupted druids guard a dark gem needed to lift the vineyard’s curse. :\\n\"\n",
    "            \"At their journey’s culmination, the PCs storm Castle Ravenloft’s shadowed halls, confront Strahd amid gothic spires, and challenge his vampire spawn and brides in a climactic battle for Barovia’s freedom. :\\n\"\n",
    "            \"The module’s gothic ambiance is enriched by haunting music cues, Tarokka card handouts, and richly illustrated maps that deepen immersion in Barovia’s despair. :\\n\"\n",
    "            \"This is the retrieved context:\\n\\n\"\n",
    "            f\"{state['context']}\\n\\n\"\n",
    "            \"You must follow the these rules strictly\\n\\n \"\n",
    "            \"1. DO NOT simulate the result of player\\'s actions.\\n\\n\"\n",
    "            \"2.Do not include <CHARACTER>.\\n\\n\"\n",
    "            \"3.Use the context provided.\\n\\n\"\n",
    "            \"YOU MUST ALSO USE these current game state data :\"\n",
    "            \"Current location: Vallaki’s forced festivals of joy\"\n",
    "            # \"When you answer the player, you must respond in proper markdown format: heading, table, bold, italic, paragraph, blockquotes.\\n\"\n",
=======
    "            \"<|start_header_id|>system<|end_header_id|>\\n\"\n",
    "            \"In a text-based adventure (Dungeons and Dragons), your job is to narrate the adventure \"\n",
    "            \"and respond to the player's actions.\\n\"\n",
    "            \"Use the following pieces of retrieved context to answer the question.\\n\"\n",
    "            \"If you don't know the answer, say i dont know.\"\n",
    "            \"If the player breaks the game rules, \"\n",
    "            \"notify the player.\\n\"\n",
    "            \"This is the retrieved context:\\n\\n\"\n",
    "            f\"{state['context']}\\n\\n\"\n",
    "            \"When you answer the player, you must respond in proper markdown format: heading, table, bold, italic, paragraph, blockquotes.\\n\"\n",
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
    "        )\n",
    "\n",
    "        print('State:', state)\n",
    "        print('Context:', state[\"context\"])\n",
    "        print('Generating response ...')\n",
    "\n",
    "        conversation_messages = [\n",
    "            message\n",
    "            for message in state[\"messages\"]\n",
    "            if message.type in (\"human\", \"system\")\n",
    "            or (message.type == \"ai\" and not message.tool_calls)\n",
    "        ]\n",
    "\n",
    "        prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "        response = self.chat.invoke(prompt)\n",
    "        \n",
    "        return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee396ba",
   "metadata": {},
   "source": [
    "### LlamaChat Class Usage"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 13,
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
   "id": "3c3d8fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "339c5cecd76a4a31bf56a1205bcf9b81",
=======
       "model_id": "68ebab9a95bb40319aeef0b817f14f73",
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "/home/yuaylong/miniconda3/envs/nlp/lib/python3.12/site-packages/peft/peft_model.py:569: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_magnitude_vector.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_magnitude_vector.default.weight'].\n",
      "  warnings.warn(warn_message)\n",
      "/home/yuaylong/miniconda3/envs/nlp/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
=======
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "llama_model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
=======
    "llama_model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
    "\n",
    "llama = LlamaChat(model_name=llama_model_name)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
   "id": "7ae600af",
   "metadata": {},
   "outputs": [],
   "source": [
=======
   "execution_count": null,
   "id": "7ae600af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: {'name': 'player_1', 'messages': [HumanMessage(content='<|start_header_id|>player_1<|end_header_id|>\\nCan I cast meteor swarm?<|eot_id|>', additional_kwargs={}, response_metadata={})], 'context': ''}\n",
      "Context: \n",
      "Generating response ...\n",
      "{'messages': [AIMessage(content=\"**Meteor Swarm**\\n================\\n\\nYou can't cast Meteor Swarm. Meteor Swarm is a 5th-level evocation spell, and you are currently a 2nd-level sorcerer. You need to reach 5th level to cast this spell.\\n\\n**Your Current Spells**\\n------------------------\\n\\n* **Magic Missile** (1st level evocation)\\n* **Shield** (1st level abjuration)\\n\\nYou can try to cast a different spell or use your spell slots to learn new spells.\", additional_kwargs={}, response_metadata={}, id='run--e75f89bd-9046-4b77-9d5e-0b42809ca849-0')]}\n"
     ]
    }
   ],
   "source": [
    "# prompt = (\n",
    "#             \"<|start_header_id|>system<|end_header_id|>\\n\"\n",
    "#             \"You are tasked with generating a relevant command for a tool-calling model.\\n\" \\\n",
    "#             \"Your answer should be a simple sentence.\\n\" \\\n",
    "#         )\n",
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
    "# text = \"<|start_header_id|>player_1<|end_header_id|>\\nCan I cast meteor swarm?<|eot_id|>\"\n",
    "\n",
    "# input_state = {\n",
    "#     \"name\": \"player_1\",\n",
    "#     \"messages\": [\n",
    "#         HumanMessage(content=text)\n",
    "#     ],\n",
    "#     \"context\": \"\"\n",
    "# }\n",
    "\n",
    "# response = llama.generate(state=input_state)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21519ef",
   "metadata": {},
   "source": [
    "## LangGraph Demo"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 15,
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
   "id": "3bf6853e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tool(state: State):\n",
    "    return tool_calling.invoke(state)\n",
    "\n",
    "def chatbot(state: State) -> str:\n",
    "    return llama.generate(state=state)\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "graph = StateGraph(State)\n",
    "\n",
    "graph.add_edge(START, \"tool call\")\n",
    "graph.add_node(\"tool call\", tool)\n",
    "\n",
    "graph.add_edge(\"tool call\", \"chatbot\")\n",
    "\n",
    "graph.add_node(\"chatbot\", chatbot)\n",
    "graph.add_edge(\"chatbot\", END)\n",
    "\n",
    "graph = graph.compile(checkpointer=memory)\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
=======
   "execution_count": 16,
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
   "id": "9c77b85d",
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "<|start_header_id|>player_3<|end_header_id|>\n",
      "Currently I am level 1 wizard.\n",
      "Can I cast meteor swarm?<|eot_id|>\n"
     ]
    },
    {
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool call request: [{'name': 'spell_retrieve', 'arguments': {'query': 'meteor swarm'}}]\n",
<<<<<<< HEAD
      "State: {'name': 'player_1', 'messages': [HumanMessage(content='<|start_header_id|>player_3<|end_header_id|>\\nCurrently I am level 1 wizard.\\nCan I cast meteor swarm?<|eot_id|>', additional_kwargs={}, response_metadata={}, id='489af465-4af6-4d01-8ce1-3ea2deac2dfe')], 'context': AIMessage(content=[\"# Meteor Swarm\\n## Spell Name\\nMeteor Swarm  \\n![](https://static.wikia.nocookie.net/dnd-5e/images/9/99/Meteor.png/revision/latest/scale-to-width-down/225?cb=20221130233621)\\n![](https://static.wikia.nocookie.net/dnd-5e/images/9/99/Meteor.png/revision/latest/scale-to-width-down/225?cb=20221130233621)  \\nFrom Player's Handbook, page 259.\\n## Description\\n*9th-level evocation*\\n* **Casting Time:** 1 action\\n* **Range:** 1 mile\\n* **Components:** V, S\\n* **Duration:** Instantaneous\\n- **Casting Time:** 1 action\\n\\n**At Higher Levels:** When you cast this spell using a spell slot of 4th level or higher, the number of meteors created increases by two for each slot level above 3rd.\\n**At Higher Levels:**\\n## Learned By\\n* **Classes:** Sorcerer, Wizard\\n* **Subclasses:** Fighter (*Eldritch Knight*), Rogue (*Arcane Trickster*)\\n- **Classes:** Sorcerer, Wizard\\n**Classes:**\\nSorcerer\\nWizard\\n- **Subclasses:** Fighter (*Eldritch Knight*), Rogue (*Arcane Trickster*)\\n**Subclasses:**\\n*Eldritch Knight*\\nEldritch Knight\\n\\n**Components:**\\n- **Duration:** Concentration, up to 10 minutes\\n**Duration:**\\nSwarming, biting locusts fill a 20-foot-radius sphere centered on a point you choose within range. The sphere spreads around corners. The sphere remains for the duration, and its area is lightly obscured. The sphere's area is difficult terrain.\"], additional_kwargs={}, response_metadata={})}\n",
      "Context: content=[\"# Meteor Swarm\\n## Spell Name\\nMeteor Swarm  \\n![](https://static.wikia.nocookie.net/dnd-5e/images/9/99/Meteor.png/revision/latest/scale-to-width-down/225?cb=20221130233621)\\n![](https://static.wikia.nocookie.net/dnd-5e/images/9/99/Meteor.png/revision/latest/scale-to-width-down/225?cb=20221130233621)  \\nFrom Player's Handbook, page 259.\\n## Description\\n*9th-level evocation*\\n* **Casting Time:** 1 action\\n* **Range:** 1 mile\\n* **Components:** V, S\\n* **Duration:** Instantaneous\\n- **Casting Time:** 1 action\\n\\n**At Higher Levels:** When you cast this spell using a spell slot of 4th level or higher, the number of meteors created increases by two for each slot level above 3rd.\\n**At Higher Levels:**\\n## Learned By\\n* **Classes:** Sorcerer, Wizard\\n* **Subclasses:** Fighter (*Eldritch Knight*), Rogue (*Arcane Trickster*)\\n- **Classes:** Sorcerer, Wizard\\n**Classes:**\\nSorcerer\\nWizard\\n- **Subclasses:** Fighter (*Eldritch Knight*), Rogue (*Arcane Trickster*)\\n**Subclasses:**\\n*Eldritch Knight*\\nEldritch Knight\\n\\n**Components:**\\n- **Duration:** Concentration, up to 10 minutes\\n**Duration:**\\nSwarming, biting locusts fill a 20-foot-radius sphere centered on a point you choose within range. The sphere spreads around corners. The sphere remains for the duration, and its area is lightly obscured. The sphere's area is difficult terrain.\"] additional_kwargs={} response_metadata={}\n",
      "Generating response ...\n",
      "You can cast Meteor Swarm, but be aware that as a level 1 wizard, you do not have access to the spell. Meteor Swarm is a 9th-level evocation spell, which means you are still far from being able to cast it.\n",
      "\n",
      "As you browse through the dusty tomes in the Baron's library, you overhear whispers of a mysterious gathering in the town square. The townspeople seem to be preparing for the Vallaki's forced festivals of joy, but there's an undercurrent of unease in the air. The Baron's guards are on high alert, and the air is thick with the smell of roasting meats and freshly baked bread.\n",
      "\n",
      "You notice a group of traveling performers, including a juggler, a musician, and a fire dancer, setting up their acts near the town square. The crowd is growing, and the atmosphere is tense. What would you like to do?\n"
=======
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "<|start_header_id|>player_3<|end_header_id|>\n",
      "Currently I am level 1 wizard.\n",
      "Can I cast meteor swarm?<|eot_id|>\n",
      "State: {'name': 'player_1', 'messages': [HumanMessage(content='<|start_header_id|>player_3<|end_header_id|>\\nCurrently I am level 1 wizard.\\nCan I cast meteor swarm?<|eot_id|>', additional_kwargs={}, response_metadata={}, id='67f82023-0478-4006-beda-5c0cb2c2579e')], 'context': AIMessage(content=[\"# Meteor Swarm\\n## Spell Name\\nMeteor Swarm  \\n![](https://static.wikia.nocookie.net/dnd-5e/images/9/99/Meteor.png/revision/latest/scale-to-width-down/225?cb=20221130233621)\\n![](https://static.wikia.nocookie.net/dnd-5e/images/9/99/Meteor.png/revision/latest/scale-to-width-down/225?cb=20221130233621)  \\nFrom Player's Handbook, page 259.\\n## Description\\n*9th-level evocation*\\n* **Casting Time:** 1 action\\n* **Range:** 1 mile\\n* **Components:** V, S\\n* **Duration:** Instantaneous\\n- **Casting Time:** 1 action\\n\\n**At Higher Levels:** When you cast this spell using a spell slot of 4th level or higher, the number of meteors created increases by two for each slot level above 3rd.\\n**At Higher Levels:**\\n## Learned By\\n* **Classes:** Sorcerer, Wizard\\n* **Subclasses:** Fighter (*Eldritch Knight*), Rogue (*Arcane Trickster*)\\n- **Classes:** Sorcerer, Wizard\\n**Classes:**\\nSorcerer\\nWizard\\n- **Subclasses:** Fighter (*Eldritch Knight*), Rogue (*Arcane Trickster*)\\n**Subclasses:**\\n*Eldritch Knight*\\nEldritch Knight\\n\\n**Components:**\\n- **Duration:** Concentration, up to 10 minutes\\n**Duration:**\\nSwarming, biting locusts fill a 20-foot-radius sphere centered on a point you choose within range. The sphere spreads around corners. The sphere remains for the duration, and its area is lightly obscured. The sphere's area is difficult terrain.\"], additional_kwargs={}, response_metadata={})}\n",
      "Context: content=[\"# Meteor Swarm\\n## Spell Name\\nMeteor Swarm  \\n![](https://static.wikia.nocookie.net/dnd-5e/images/9/99/Meteor.png/revision/latest/scale-to-width-down/225?cb=20221130233621)\\n![](https://static.wikia.nocookie.net/dnd-5e/images/9/99/Meteor.png/revision/latest/scale-to-width-down/225?cb=20221130233621)  \\nFrom Player's Handbook, page 259.\\n## Description\\n*9th-level evocation*\\n* **Casting Time:** 1 action\\n* **Range:** 1 mile\\n* **Components:** V, S\\n* **Duration:** Instantaneous\\n- **Casting Time:** 1 action\\n\\n**At Higher Levels:** When you cast this spell using a spell slot of 4th level or higher, the number of meteors created increases by two for each slot level above 3rd.\\n**At Higher Levels:**\\n## Learned By\\n* **Classes:** Sorcerer, Wizard\\n* **Subclasses:** Fighter (*Eldritch Knight*), Rogue (*Arcane Trickster*)\\n- **Classes:** Sorcerer, Wizard\\n**Classes:**\\nSorcerer\\nWizard\\n- **Subclasses:** Fighter (*Eldritch Knight*), Rogue (*Arcane Trickster*)\\n**Subclasses:**\\n*Eldritch Knight*\\nEldritch Knight\\n\\n**Components:**\\n- **Duration:** Concentration, up to 10 minutes\\n**Duration:**\\nSwarming, biting locusts fill a 20-foot-radius sphere centered on a point you choose within range. The sphere spreads around corners. The sphere remains for the duration, and its area is lightly obscured. The sphere's area is difficult terrain.\"] additional_kwargs={} response_metadata={}\n",
      "Generating response ...\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Not Yet!**\n",
      "As a level 1 wizard, you don't meet the prerequisites to cast Meteor Swarm, which is a 9th-level evocation spell. The spell requires a caster level of at least 9th level to cast, and you're still a lowly 1st level wizard. You'll need to gain more experience and level up to become eligible to cast this impressive spell!\n"
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
     ]
    }
   ],
   "source": [
    "thread_id = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": \"123\"}}\n",
    "\n",
    "text = \"<|start_header_id|>player_3<|end_header_id|>\\nCurrently I am level 1 wizard.\\nCan I cast meteor swarm?<|eot_id|>\"\n",
    "\n",
    "input_state = {\n",
    "    \"name\": \"player_1\",\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=text)\n",
    "    ],\n",
    "    \"context\": \"\"\n",
    "}\n",
    "\n",
<<<<<<< HEAD
    "# for step in graph.stream(\n",
    "#     input_state,\n",
    "#     config=config,\n",
    "#     stream_mode=\"values\",\n",
    "# ):\n",
    "#     step[\"messages\"][-1].pretty_print()\n",
    "\n",
    "response = graph.invoke(input_state, config)\n",
    "\n",
    "print(response['messages'][-1].content)"
=======
    "for step in graph.stream(\n",
    "    input_state,\n",
    "    config=config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": 17,
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
   "id": "b49b16f6",
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "<|start_header_id|>player_3<|end_header_id|>\n",
      "Can you give me infomation of name User3?<|eot_id|>\n"
     ]
    },
    {
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool call request: [{'name': 'user', 'arguments': {'name': 'User3'}}]\n",
<<<<<<< HEAD
      "State: {'name': 'player_1', 'messages': [HumanMessage(content='<|start_header_id|>player_3<|end_header_id|>\\nCurrently I am level 1 wizard.\\nCan I cast meteor swarm?<|eot_id|>', additional_kwargs={}, response_metadata={}, id='489af465-4af6-4d01-8ce1-3ea2deac2dfe'), AIMessage(content=\"You can cast Meteor Swarm, but be aware that as a level 1 wizard, you do not have access to the spell. Meteor Swarm is a 9th-level evocation spell, which means you are still far from being able to cast it.\\n\\nAs you browse through the dusty tomes in the Baron's library, you overhear whispers of a mysterious gathering in the town square. The townspeople seem to be preparing for the Vallaki's forced festivals of joy, but there's an undercurrent of unease in the air. The Baron's guards are on high alert, and the air is thick with the smell of roasting meats and freshly baked bread.\\n\\nYou notice a group of traveling performers, including a juggler, a musician, and a fire dancer, setting up their acts near the town square. The crowd is growing, and the atmosphere is tense. What would you like to do?\", additional_kwargs={}, response_metadata={}, id='run--fe0dc82a-9e38-4a43-8f3b-74a4c8f06070-0'), HumanMessage(content='<|start_header_id|>player_3<|end_header_id|>\\nCan you give me infomation of name User3?<|eot_id|>', additional_kwargs={}, response_metadata={}, id='4ce0f2c8-810c-46cd-8835-aa26fcd339f5')], 'context': AIMessage(content=['  user_id   name  age              email  num_posts  num_likes  num_followers  \\\\\\n2  user_3  User3   57  user3@example.com         46       8255           1651   \\n\\n   num_following  \\n2           2249  .'], additional_kwargs={}, response_metadata={})}\n",
      "Context: content=['  user_id   name  age              email  num_posts  num_likes  num_followers  \\\\\\n2  user_3  User3   57  user3@example.com         46       8255           1651   \\n\\n   num_following  \\n2           2249  .'] additional_kwargs={} response_metadata={}\n",
      "Generating response ...\n",
      "You are User_3, a level 1 wizard, with an age of 57 and an email address of user3@example.com. You have made 46 posts, received 8255 likes, and have 1651 followers. You are currently following 2 people.\n",
      "\n",
      "You are in the midst of the Vallaki's forced festivals of joy in the town square. The tension is palpable, and the air is filled with the smell of roasting meats and freshly baked bread. The crowd is growing, and the traveling performers are setting up their acts.\n",
      "\n",
      "As you observe the scene, you notice a group of guardsmen eyeing you suspiciously. They seem to be watching you with an air of curiosity, and you wonder if you've attracted their attention for some reason.\n",
      "\n",
      "The festival's festivities are about to begin, and the Baron's voice booms through the speakers, announcing the start of the celebrations. The crowd erupts into cheers and applause, and the performers take their places. The juggler begins to juggle clubs, the musician starts to play a lively tune, and the fire dancer spins and leaps through the flames.\n",
      "\n",
      "What would you like to do next?\n"
=======
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "<|start_header_id|>player_3<|end_header_id|>\n",
      "Can you give me infomation of name User3?<|eot_id|>\n",
      "State: {'name': 'player_1', 'messages': [HumanMessage(content='<|start_header_id|>player_3<|end_header_id|>\\nCurrently I am level 1 wizard.\\nCan I cast meteor swarm?<|eot_id|>', additional_kwargs={}, response_metadata={}, id='67f82023-0478-4006-beda-5c0cb2c2579e'), AIMessage(content=\"**Not Yet!**\\nAs a level 1 wizard, you don't meet the prerequisites to cast Meteor Swarm, which is a 9th-level evocation spell. The spell requires a caster level of at least 9th level to cast, and you're still a lowly 1st level wizard. You'll need to gain more experience and level up to become eligible to cast this impressive spell!\", additional_kwargs={}, response_metadata={}, id='run--2506e5b3-e052-403b-803f-f4f57adb1a35-0'), HumanMessage(content='<|start_header_id|>player_3<|end_header_id|>\\nCan you give me infomation of name User3?<|eot_id|>', additional_kwargs={}, response_metadata={}, id='36156742-af8a-4658-b8e4-fa8b64f28fb3')], 'context': AIMessage(content=['  user_id   name  age              email  num_posts  num_likes  num_followers  \\\\\\n2  user_3  User3   50  user3@example.com         13       1779          51522   \\n\\n   num_following  \\n2           4200  .'], additional_kwargs={}, response_metadata={})}\n",
      "Context: content=['  user_id   name  age              email  num_posts  num_likes  num_followers  \\\\\\n2  user_3  User3   50  user3@example.com         13       1779          51522   \\n\\n   num_following  \\n2           4200  .'] additional_kwargs={} response_metadata={}\n",
      "Generating response ...\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**User Information**\n",
      "Here's the information you requested:\n",
      "\n",
      "| **Name** | **Age** | **Email** | **Number of Posts** | **Number of Likes** | **Number of Followers** |\n",
      "| --- | --- | --- | --- | --- | --- |\n",
      "| **User3** | 50 | user3@example.com | 13 | 1779 | 51522 |\n",
      "\n",
      "Note: This information seems to be unrelated to your character's stats or progress in the game. It appears to be personal information about a user named User3.\n"
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
     ]
    }
   ],
   "source": [
    "text = \"<|start_header_id|>player_3<|end_header_id|>\\nCan you give me infomation of name User3?<|eot_id|>\"\n",
    "\n",
    "input_state = {\n",
    "    \"name\": \"player_1\",\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=text)\n",
    "    ],\n",
    "    \"context\": \"\"\n",
    "}\n",
    "\n",
<<<<<<< HEAD
    "# for step in graph.stream(\n",
    "#     input_state,\n",
    "#     config=config,\n",
    "#     stream_mode=\"values\",\n",
    "# ):\n",
    "#     step[\"messages\"][-1].pretty_print()\n",
    "\n",
    "\n",
    "response = graph.invoke(input_state, config)\n",
    "\n",
    "print(response['messages'][-1].content)"
=======
    "for step in graph.stream(\n",
    "    input_state,\n",
    "    config=config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 18,
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
   "id": "045ecb72",
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool call request: [{'name': 'user', 'arguments': {'name': 'Tsubasa'}}]\n",
      "State: {'name': 'player_1', 'messages': [HumanMessage(content='<|start_header_id|>player_3<|end_header_id|>\\nCurrently I am level 1 wizard.\\nCan I cast meteor swarm?<|eot_id|>', additional_kwargs={}, response_metadata={}, id='489af465-4af6-4d01-8ce1-3ea2deac2dfe'), AIMessage(content=\"You can cast Meteor Swarm, but be aware that as a level 1 wizard, you do not have access to the spell. Meteor Swarm is a 9th-level evocation spell, which means you are still far from being able to cast it.\\n\\nAs you browse through the dusty tomes in the Baron's library, you overhear whispers of a mysterious gathering in the town square. The townspeople seem to be preparing for the Vallaki's forced festivals of joy, but there's an undercurrent of unease in the air. The Baron's guards are on high alert, and the air is thick with the smell of roasting meats and freshly baked bread.\\n\\nYou notice a group of traveling performers, including a juggler, a musician, and a fire dancer, setting up their acts near the town square. The crowd is growing, and the atmosphere is tense. What would you like to do?\", additional_kwargs={}, response_metadata={}, id='run--fe0dc82a-9e38-4a43-8f3b-74a4c8f06070-0'), HumanMessage(content='<|start_header_id|>player_3<|end_header_id|>\\nCan you give me infomation of name User3?<|eot_id|>', additional_kwargs={}, response_metadata={}, id='4ce0f2c8-810c-46cd-8835-aa26fcd339f5'), AIMessage(content=\"You are User_3, a level 1 wizard, with an age of 57 and an email address of user3@example.com. You have made 46 posts, received 8255 likes, and have 1651 followers. You are currently following 2 people.\\n\\nYou are in the midst of the Vallaki's forced festivals of joy in the town square. The tension is palpable, and the air is filled with the smell of roasting meats and freshly baked bread. The crowd is growing, and the traveling performers are setting up their acts.\\n\\nAs you observe the scene, you notice a group of guardsmen eyeing you suspiciously. They seem to be watching you with an air of curiosity, and you wonder if you've attracted their attention for some reason.\\n\\nThe festival's festivities are about to begin, and the Baron's voice booms through the speakers, announcing the start of the celebrations. The crowd erupts into cheers and applause, and the performers take their places. The juggler begins to juggle clubs, the musician starts to play a lively tune, and the fire dancer spins and leaps through the flames.\\n\\nWhat would you like to do next?\", additional_kwargs={}, response_metadata={}, id='run--b91adb1e-103f-413b-945e-ea39571ca825-0'), HumanMessage(content='<|start_header_id|>player1<|end_header_id|>\\nMy name is Tsubasa.<|eot_id|>', additional_kwargs={}, response_metadata={}, id='177d1d57-3502-4979-96e3-3c9af71ec298')], 'context': AIMessage(content=['Empty DataFrame\\nColumns: [user_id, name, age, email, num_posts, num_likes, num_followers, num_following]\\nIndex: [].'], additional_kwargs={}, response_metadata={})}\n",
      "Context: content=['Empty DataFrame\\nColumns: [user_id, name, age, email, num_posts, num_likes, num_followers, num_following]\\nIndex: [].'] additional_kwargs={} response_metadata={}\n",
      "Generating response ...\n",
      "You, Tsubasa, are a character in the midst of the Vallaki's forced festivals of joy. You are not currently in the town square, but you are somehow connected to the scene.\n",
      "\n",
      "As the festival begins, the air is filled with the sounds of music and laughter. The townspeople are trying to put on a brave face, but the tension beneath the surface is palpable. You sense that something is off, and the atmosphere is thick with unease.\n",
      "\n",
      "You find yourself in a nearby tavern, the Red Griffin Inn. The fire is crackling, and the patrons are huddled in small groups, speaking in hushed tones. The bard in the corner raises her voice, singing a lively tune that seems to be a deliberate attempt to lift the spirits of the crowd.\n",
      "\n",
      "As you take a seat at the bar, the bard notices you and begins to sing a song that seems to be directed at you. The lyrics speak of a hero who has come to Vallaki, and the bard's eyes seem to lock onto yours, as if she knows you're the one she's singing about.\n",
      "\n",
      "The barkeep greets you with a friendly smile, offering you a mug of ale. What would you like to do?\n"
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "<|start_header_id|>player1<|end_header_id|>\n",
      "My name is Tsubasa.<|eot_id|>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m input_message = \u001b[33m\"\u001b[39m\u001b[33m<|start_header_id|>player1<|end_header_id|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMy name is Tsubasa.<|eot_id|>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m input_state = {\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mplayer_1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpretty_print\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2340\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[39m\n\u001b[32m   2334\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2335\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2336\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2337\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2338\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2339\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2340\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2341\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2342\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2343\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2344\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2345\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2346\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2347\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2348\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/langgraph/pregel/runner.py:158\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    156\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/langgraph/pregel/retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/langgraph/utils/runnable.py:606\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m config = patch_config(\n\u001b[32m    603\u001b[39m     config, callbacks=run_manager.get_child(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    604\u001b[39m )\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    608\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/langgraph/utils/runnable.py:371\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m         ret = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse:\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mtool\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtool\u001b[39m(state: State):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtool_calling\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mToolCalling.invoke\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m     72\u001b[39m messages = [system_prompt] + [state[\u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m]]\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     response = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     77\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mError invoking tool:\u001b[39m\u001b[33m\"\u001b[39m, e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/langchain_core/runnables/base.py:3032\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3030\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3031\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3032\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3033\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3034\u001b[39m         \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:370\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    360\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    365\u001b[39m     **kwargs: Any,\n\u001b[32m    366\u001b[39m ) -> BaseMessage:\n\u001b[32m    367\u001b[39m     config = ensure_config(config)\n\u001b[32m    368\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    369\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    380\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:947\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    939\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    940\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    944\u001b[39m     **kwargs: Any,\n\u001b[32m    945\u001b[39m ) -> LLMResult:\n\u001b[32m    946\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:766\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    764\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    765\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    772\u001b[39m         )\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    774\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1012\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1010\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1016\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/langchain_huggingface/chat_models/huggingface.py:374\u001b[39m, in \u001b[36mChatHuggingFace._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    373\u001b[39m     llm_input = \u001b[38;5;28mself\u001b[39m._to_chat_prompt(messages)\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m     llm_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mllm_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._to_chat_result(llm_result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/langchain_huggingface/llms/huggingface_pipeline.py:295\u001b[39m, in \u001b[36mHuggingFacePipeline._generate\u001b[39m\u001b[34m(self, prompts, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    292\u001b[39m batch_prompts = prompts[i : i + \u001b[38;5;28mself\u001b[39m.batch_size]\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# Process batch of prompts\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m responses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpipeline_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[38;5;66;03m# Process each response in the batch\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j, response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(responses):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:287\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    286\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/transformers/pipelines/base.py:1360\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[32m   1357\u001b[39m     final_iterator = \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   1358\u001b[39m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[32m   1359\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m     outputs = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1361\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:124\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_item()\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m item = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m processed = \u001b[38;5;28mself\u001b[39m.infer(item, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:125\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m    124\u001b[39m item = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m processed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    128\u001b[39m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/transformers/pipelines/base.py:1286\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1284\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1285\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1286\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1287\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/transformers/pipelines/text_generation.py:385\u001b[39m, in \u001b[36mTextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    383\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[32m    388\u001b[39m     generated_sequence = output.sequences\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/transformers/generation/utils.py:2465\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2457\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2458\u001b[39m         input_ids=input_ids,\n\u001b[32m   2459\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2460\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2461\u001b[39m         **model_kwargs,\n\u001b[32m   2462\u001b[39m     )\n\u001b[32m   2464\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2465\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2470\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2472\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2476\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2477\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2478\u001b[39m         input_ids=input_ids,\n\u001b[32m   2479\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2480\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2481\u001b[39m         **model_kwargs,\n\u001b[32m   2482\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/transformers/generation/utils.py:3434\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3432\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3434\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3436\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3437\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3438\u001b[39m     outputs,\n\u001b[32m   3439\u001b[39m     model_kwargs,\n\u001b[32m   3440\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3441\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:821\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    816\u001b[39m output_hidden_states = (\n\u001b[32m    817\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    818\u001b[39m )\n\u001b[32m    820\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    834\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    835\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:571\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    559\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    560\u001b[39m         partial(decoder_layer.\u001b[34m__call__\u001b[39m, **flash_attn_kwargs),\n\u001b[32m    561\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    568\u001b[39m         position_embeddings,\n\u001b[32m    569\u001b[39m     )\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:318\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    315\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    317\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    329\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nlp/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:253\u001b[39m, in \u001b[36mLlamaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    250\u001b[39m hidden_shape = (*input_shape, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.head_dim)\n\u001b[32m    252\u001b[39m query_states = \u001b[38;5;28mself\u001b[39m.q_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m key_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m.view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    254\u001b[39m value_states = \u001b[38;5;28mself\u001b[39m.v_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    256\u001b[39m cos, sin = position_embeddings\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
     ]
    }
   ],
   "source": [
    "input_message = \"<|start_header_id|>player1<|end_header_id|>\\nMy name is Tsubasa.<|eot_id|>\"\n",
    "\n",
    "input_state = {\n",
    "    \"name\": \"player_1\",\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=input_message)\n",
    "    ],\n",
    "    \"context\": \"\"\n",
    "}\n",
    "\n",
<<<<<<< HEAD
    "# for step in graph.stream(\n",
    "#     input_state,\n",
    "#     config=config,\n",
    "#     stream_mode=\"values\",\n",
    "# ):\n",
    "#     step[\"messages\"][-1].pretty_print()\n",
    "\n",
    "\n",
    "response = graph.invoke(input_state, config)\n",
    "\n",
    "print(response['messages'][-1].content)"
=======
    "for step in graph.stream(\n",
    "    input_state,\n",
    "    config=config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": null,
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
   "id": "d163be2d",
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "<|start_header_id|>player1<|end_header_id|>\n",
      "hello, What is my name?<|eot_id|>\n"
     ]
    },
    {
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool call request: [{'name': 'user', 'arguments': {'name': 'player1'}}]\n",
<<<<<<< HEAD
      "State: {'name': 'player_1', 'messages': [HumanMessage(content='<|start_header_id|>player_3<|end_header_id|>\\nCurrently I am level 1 wizard.\\nCan I cast meteor swarm?<|eot_id|>', additional_kwargs={}, response_metadata={}, id='489af465-4af6-4d01-8ce1-3ea2deac2dfe'), AIMessage(content=\"You can cast Meteor Swarm, but be aware that as a level 1 wizard, you do not have access to the spell. Meteor Swarm is a 9th-level evocation spell, which means you are still far from being able to cast it.\\n\\nAs you browse through the dusty tomes in the Baron's library, you overhear whispers of a mysterious gathering in the town square. The townspeople seem to be preparing for the Vallaki's forced festivals of joy, but there's an undercurrent of unease in the air. The Baron's guards are on high alert, and the air is thick with the smell of roasting meats and freshly baked bread.\\n\\nYou notice a group of traveling performers, including a juggler, a musician, and a fire dancer, setting up their acts near the town square. The crowd is growing, and the atmosphere is tense. What would you like to do?\", additional_kwargs={}, response_metadata={}, id='run--fe0dc82a-9e38-4a43-8f3b-74a4c8f06070-0'), HumanMessage(content='<|start_header_id|>player_3<|end_header_id|>\\nCan you give me infomation of name User3?<|eot_id|>', additional_kwargs={}, response_metadata={}, id='4ce0f2c8-810c-46cd-8835-aa26fcd339f5'), AIMessage(content=\"You are User_3, a level 1 wizard, with an age of 57 and an email address of user3@example.com. You have made 46 posts, received 8255 likes, and have 1651 followers. You are currently following 2 people.\\n\\nYou are in the midst of the Vallaki's forced festivals of joy in the town square. The tension is palpable, and the air is filled with the smell of roasting meats and freshly baked bread. The crowd is growing, and the traveling performers are setting up their acts.\\n\\nAs you observe the scene, you notice a group of guardsmen eyeing you suspiciously. They seem to be watching you with an air of curiosity, and you wonder if you've attracted their attention for some reason.\\n\\nThe festival's festivities are about to begin, and the Baron's voice booms through the speakers, announcing the start of the celebrations. The crowd erupts into cheers and applause, and the performers take their places. The juggler begins to juggle clubs, the musician starts to play a lively tune, and the fire dancer spins and leaps through the flames.\\n\\nWhat would you like to do next?\", additional_kwargs={}, response_metadata={}, id='run--b91adb1e-103f-413b-945e-ea39571ca825-0'), HumanMessage(content='<|start_header_id|>player1<|end_header_id|>\\nMy name is Tsubasa.<|eot_id|>', additional_kwargs={}, response_metadata={}, id='177d1d57-3502-4979-96e3-3c9af71ec298'), AIMessage(content=\"You, Tsubasa, are a character in the midst of the Vallaki's forced festivals of joy. You are not currently in the town square, but you are somehow connected to the scene.\\n\\nAs the festival begins, the air is filled with the sounds of music and laughter. The townspeople are trying to put on a brave face, but the tension beneath the surface is palpable. You sense that something is off, and the atmosphere is thick with unease.\\n\\nYou find yourself in a nearby tavern, the Red Griffin Inn. The fire is crackling, and the patrons are huddled in small groups, speaking in hushed tones. The bard in the corner raises her voice, singing a lively tune that seems to be a deliberate attempt to lift the spirits of the crowd.\\n\\nAs you take a seat at the bar, the bard notices you and begins to sing a song that seems to be directed at you. The lyrics speak of a hero who has come to Vallaki, and the bard's eyes seem to lock onto yours, as if she knows you're the one she's singing about.\\n\\nThe barkeep greets you with a friendly smile, offering you a mug of ale. What would you like to do?\", additional_kwargs={}, response_metadata={}, id='run--ee70d782-0af9-4f0c-ab1a-325c01181021-0'), HumanMessage(content='<|start_header_id|>player1<|end_header_id|>\\nhello, What is my name?<|eot_id|>', additional_kwargs={}, response_metadata={}, id='936cbd6b-3b8f-44a4-adca-a648bd1c1092')], 'context': AIMessage(content=['Empty DataFrame\\nColumns: [user_id, name, age, email, num_posts, num_likes, num_followers, num_following]\\nIndex: [].'], additional_kwargs={}, response_metadata={})}\n",
      "Context: content=['Empty DataFrame\\nColumns: [user_id, name, age, email, num_posts, num_likes, num_followers, num_following]\\nIndex: [].'] additional_kwargs={} response_metadata={}\n",
      "Generating response ...\n",
      "The bard's song comes to an abrupt end as she looks at you with a mixture of curiosity and concern. \"Ah, you're the one they're talking about, I suppose,\" she says, her voice low and cautious. \"My name is Elara, and I'm the bard for the Baron's festival. I was just... singing about the stranger who's come to Vallaki.\"\n",
      "\n",
      "She leans in closer, her voice taking on a conspiratorial tone. \"If you're the one I'm thinking of, you should be careful. The Baron's not one to be trifled with. He's... watching everyone closely, and anyone who attracts attention is of interest to him.\"\n",
      "\n",
      "Elara glances around the tavern, making sure no one is listening in. \"If you want to talk, I'm happy to share what I know. But be quick, and be careful.\"\n",
      "\n",
      "The patrons of the tavern seem to be watching you with interest, their conversations dying down as they sense the tension in the air. The barkeep clears his throat, reminding you that you're a guest in his establishment.\n",
      "\n",
      "What would you like to do?\n"
=======
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "<|start_header_id|>player1<|end_header_id|>\n",
      "hello, What is my name?<|eot_id|>\n",
      "State: {'name': 'player_1', 'messages': [HumanMessage(content='<|start_header_id|>player_3<|end_header_id|>\\nCan I cast meteor swarm?<|eot_id|>', additional_kwargs={}, response_metadata={}, id='b7b2b34e-091e-4f5e-aa93-34412cbab06c'), AIMessage(content=\"**You Can Cast Meteor Swarm!**\\n\\nAs a **Sorcerer** or **Wizard**, you have learned the **Meteor Swarm** spell, a powerful evocation that can unleash a swarm of biting locusts to fill a 20-foot-radius sphere centered on a point you choose within range.\\n\\n**Casting Requirements:**\\n\\n* **Casting Time:** 1 action\\n* **Range:** 1 mile\\n* **Components:** V, S\\n* **Duration:** Instantaneous\\n\\n**At Higher Levels:** When you cast this spell using a spell slot of 4th level or higher, the number of meteors created increases by two for each slot level above 3rd.\\n\\n**What's Next?**\\n\\nNow that you know you can cast Meteor Swarm, what would you like to do?\", additional_kwargs={}, response_metadata={}, id='run--24c0dcb2-0fc7-4523-b80e-23316a61d39b-0'), HumanMessage(content='<|start_header_id|>player_3<|end_header_id|>\\nCan you give me infomation of name User3?<|eot_id|>', additional_kwargs={}, response_metadata={}, id='12853aa6-6b80-4e09-b2e6-a094753bd1ff'), AIMessage(content=\"**User Information:**\\n\\n| **Name** | **Age** | **Email** |\\n| --- | --- | --- |\\n| **User3** | **26** | user3@example.com |\\n\\n**Additional Information:**\\n\\n* **Number of Posts:** 80\\n* **Number of Likes:** 1210\\n* **Number of Followers:** 39020\\n* **Number of Following:** 114\\n\\n**What's Next?**\\n\\nNow that you have this information, what would you like to do?\", additional_kwargs={}, response_metadata={}, id='run--f75eae1a-1ea4-4092-847a-9bcd775bd3d4-0'), HumanMessage(content='<|start_header_id|>player1<|end_header_id|>\\nMy name is Tsubasa.<|eot_id|>', additional_kwargs={}, response_metadata={}, id='9e0291d2-da10-4193-a0fb-f700b92c2fe3'), AIMessage(content=\"**Welcome, Tsubasa!**\\n\\nAs a brave adventurer, you're about to embark on a thrilling journey! As a **Sorcerer**, you're skilled in the arcane arts, and you're eager to unleash your magical abilities to overcome challenges and defeat foes.\\n\\n**Your Current Situation:**\\n\\nYou find yourself in a dense forest, surrounded by towering trees that block out most of the sunlight. The air is crisp and clean, and you can hear the distant chirping of birds and the gentle rustling of leaves. A narrow path stretches out before you, disappearing into the trees. The path is overgrown with vines and shrubs, but it looks like it hasn't been traversed in a long time.\\n\\n**What Would You Like to Do?**\\n\\nAs Tsubasa, what do you want to do next? Do you want to:\\n* Follow the path to see where it leads?\\n* Search for any landmarks or distinguishing features?\\n* Use your magical abilities to scan the area for any hidden dangers?\\n* Cast a spell to illuminate the surrounding area?\\n* Rest and prepare for the journey ahead?\\n\\nChoose your response:\", additional_kwargs={}, response_metadata={}, id='run--1426dd82-cf72-403d-be5e-2354987e4633-0'), HumanMessage(content='<|start_header_id|>player1<|end_header_id|>\\nhello, What is my name?<|eot_id|>', additional_kwargs={}, response_metadata={}, id='3f128fcc-d836-44c8-bf46-04dc0b9a46a7')], 'context': AIMessage(content=['Empty DataFrame\\nColumns: [user_id, name, age, email, num_posts, num_likes, num_followers, num_following]\\nIndex: [].'], additional_kwargs={}, response_metadata={})}\n",
      "Context: content=['Empty DataFrame\\nColumns: [user_id, name, age, email, num_posts, num_likes, num_followers, num_following]\\nIndex: [].'] additional_kwargs={} response_metadata={}\n",
      "Generating response ...\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Hello, Tsubasa!**\n",
      "\n",
      "As I mentioned earlier, your name is Tsubasa. You're a brave adventurer with a knack for magic. You're currently standing in a dense forest, surrounded by towering trees that block out most of the sunlight. The air is crisp and clean, and you can hear the distant chirping of birds and the gentle rustling of leaves.\n",
      "\n",
      "**Your Current Situation:**\n",
      "\n",
      "You're still standing in the forest, surrounded by the same trees and underbrush. The path ahead of you is still overgrown with vines and shrubs, but it looks like it hasn't been traversed in a long time. You're not sure where it leads, but you're eager to find out.\n",
      "\n",
      "**What Would You Like to Do?**\n",
      "\n",
      "As Tsubasa, what do you want to do next? Do you want to:\n",
      "* Follow the path to see where it leads?\n",
      "* Search for any landmarks or distinguishing features?\n",
      "* Use your magical abilities to scan the area for any hidden dangers?\n",
      "* Cast a spell to illuminate the surrounding area?\n",
      "* Rest and prepare for the journey ahead?\n",
      "\n",
      "Choose your response:\n"
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
     ]
    }
   ],
   "source": [
    "input_message = \"<|start_header_id|>player1<|end_header_id|>\\nhello, What is my name?<|eot_id|>\"\n",
    "\n",
    "input_state = {\n",
    "    \"name\": \"player_1\",\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=input_message)\n",
    "    ],\n",
    "    \"context\": \"\"\n",
    "}\n",
    "\n",
<<<<<<< HEAD
    "# for step in graph.stream(\n",
    "#     input_state,\n",
    "#     config=config,\n",
    "#     stream_mode=\"values\",\n",
    "# ):\n",
    "#     step[\"messages\"][-1].pretty_print()\n",
    "\n",
    "\n",
    "response = graph.invoke(input_state, config)\n",
    "\n",
    "print(response['messages'][-1].content)"
=======
    "for step in graph.stream(\n",
    "    input_state,\n",
    "    config=config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
   "id": "be8fc7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool call request: [{'name': 'user', 'arguments': {'name': 'player1'}}]\n",
      "State: {'name': 'player_1', 'messages': [HumanMessage(content='<|start_header_id|>player_3<|end_header_id|>\\nCurrently I am level 1 wizard.\\nCan I cast meteor swarm?<|eot_id|>', additional_kwargs={}, response_metadata={}, id='489af465-4af6-4d01-8ce1-3ea2deac2dfe'), AIMessage(content=\"You can cast Meteor Swarm, but be aware that as a level 1 wizard, you do not have access to the spell. Meteor Swarm is a 9th-level evocation spell, which means you are still far from being able to cast it.\\n\\nAs you browse through the dusty tomes in the Baron's library, you overhear whispers of a mysterious gathering in the town square. The townspeople seem to be preparing for the Vallaki's forced festivals of joy, but there's an undercurrent of unease in the air. The Baron's guards are on high alert, and the air is thick with the smell of roasting meats and freshly baked bread.\\n\\nYou notice a group of traveling performers, including a juggler, a musician, and a fire dancer, setting up their acts near the town square. The crowd is growing, and the atmosphere is tense. What would you like to do?\", additional_kwargs={}, response_metadata={}, id='run--fe0dc82a-9e38-4a43-8f3b-74a4c8f06070-0'), HumanMessage(content='<|start_header_id|>player_3<|end_header_id|>\\nCan you give me infomation of name User3?<|eot_id|>', additional_kwargs={}, response_metadata={}, id='4ce0f2c8-810c-46cd-8835-aa26fcd339f5'), AIMessage(content=\"You are User_3, a level 1 wizard, with an age of 57 and an email address of user3@example.com. You have made 46 posts, received 8255 likes, and have 1651 followers. You are currently following 2 people.\\n\\nYou are in the midst of the Vallaki's forced festivals of joy in the town square. The tension is palpable, and the air is filled with the smell of roasting meats and freshly baked bread. The crowd is growing, and the traveling performers are setting up their acts.\\n\\nAs you observe the scene, you notice a group of guardsmen eyeing you suspiciously. They seem to be watching you with an air of curiosity, and you wonder if you've attracted their attention for some reason.\\n\\nThe festival's festivities are about to begin, and the Baron's voice booms through the speakers, announcing the start of the celebrations. The crowd erupts into cheers and applause, and the performers take their places. The juggler begins to juggle clubs, the musician starts to play a lively tune, and the fire dancer spins and leaps through the flames.\\n\\nWhat would you like to do next?\", additional_kwargs={}, response_metadata={}, id='run--b91adb1e-103f-413b-945e-ea39571ca825-0'), HumanMessage(content='<|start_header_id|>player1<|end_header_id|>\\nMy name is Tsubasa.<|eot_id|>', additional_kwargs={}, response_metadata={}, id='177d1d57-3502-4979-96e3-3c9af71ec298'), AIMessage(content=\"You, Tsubasa, are a character in the midst of the Vallaki's forced festivals of joy. You are not currently in the town square, but you are somehow connected to the scene.\\n\\nAs the festival begins, the air is filled with the sounds of music and laughter. The townspeople are trying to put on a brave face, but the tension beneath the surface is palpable. You sense that something is off, and the atmosphere is thick with unease.\\n\\nYou find yourself in a nearby tavern, the Red Griffin Inn. The fire is crackling, and the patrons are huddled in small groups, speaking in hushed tones. The bard in the corner raises her voice, singing a lively tune that seems to be a deliberate attempt to lift the spirits of the crowd.\\n\\nAs you take a seat at the bar, the bard notices you and begins to sing a song that seems to be directed at you. The lyrics speak of a hero who has come to Vallaki, and the bard's eyes seem to lock onto yours, as if she knows you're the one she's singing about.\\n\\nThe barkeep greets you with a friendly smile, offering you a mug of ale. What would you like to do?\", additional_kwargs={}, response_metadata={}, id='run--ee70d782-0af9-4f0c-ab1a-325c01181021-0'), HumanMessage(content='<|start_header_id|>player1<|end_header_id|>\\nhello, What is my name?<|eot_id|>', additional_kwargs={}, response_metadata={}, id='936cbd6b-3b8f-44a4-adca-a648bd1c1092'), AIMessage(content='The bard\\'s song comes to an abrupt end as she looks at you with a mixture of curiosity and concern. \"Ah, you\\'re the one they\\'re talking about, I suppose,\" she says, her voice low and cautious. \"My name is Elara, and I\\'m the bard for the Baron\\'s festival. I was just... singing about the stranger who\\'s come to Vallaki.\"\\n\\nShe leans in closer, her voice taking on a conspiratorial tone. \"If you\\'re the one I\\'m thinking of, you should be careful. The Baron\\'s not one to be trifled with. He\\'s... watching everyone closely, and anyone who attracts attention is of interest to him.\"\\n\\nElara glances around the tavern, making sure no one is listening in. \"If you want to talk, I\\'m happy to share what I know. But be quick, and be careful.\"\\n\\nThe patrons of the tavern seem to be watching you with interest, their conversations dying down as they sense the tension in the air. The barkeep clears his throat, reminding you that you\\'re a guest in his establishment.\\n\\nWhat would you like to do?', additional_kwargs={}, response_metadata={}, id='run--fc8f0209-1f54-42f0-87e8-c8f71f463553-0'), HumanMessage(content='<|start_header_id|>player1<|end_header_id|>\\nhello, What is my name?<|eot_id|>', additional_kwargs={}, response_metadata={}, id='a65788de-059b-4122-8dea-71d70f13bef2')], 'context': AIMessage(content=['Empty DataFrame\\nColumns: [user_id, name, age, email, num_posts, num_likes, num_followers, num_following]\\nIndex: [].'], additional_kwargs={}, response_metadata={})}\n",
      "Context: content=['Empty DataFrame\\nColumns: [user_id, name, age, email, num_posts, num_likes, num_followers, num_following]\\nIndex: [].'] additional_kwargs={} response_metadata={}\n",
      "Generating response ...\n",
      "Elara's expression turns apologetic, and she leans back, her voice dropping to a whisper. \"Ah, I'm so sorry, I didn't mean to pry. I just... you seem like someone who's been through a lot. But I don't know your name, do I?\"\n",
      "\n",
      "She pauses, studying your face, as if searching for some hidden clue. \"I'm just a bard, I don't usually get to know people's names, but... you seem different. If you want to share, I'm all ears.\"\n",
      "\n",
      "The patrons of the tavern seem to be watching the exchange with interest, their faces a mixture of curiosity and concern. The barkeep polishes a mug with a dirty rag, his eyes flicking between Elara and you.\n",
      "\n",
      "As the tension hangs in the air, a figure slips into the tavern, their eyes scanning the room before locking onto you. It's a young woman with a hood up, her face hidden in the shadows. She slips into the corner, ordering a drink without drawing attention to herself.\n",
      "\n",
      "What would you like to do?\n"
     ]
    }
   ],
   "source": [
    "input_message = \"<|start_header_id|>player1<|end_header_id|>\\nhello, What is my name?<|eot_id|>\"\n",
    "\n",
    "input_state = {\n",
    "    \"name\": \"player_1\",\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=input_message)\n",
    "    ],\n",
    "    \"context\": \"\"\n",
    "}\n",
    "\n",
    "# for step in graph.stream(\n",
    "#     input_state,\n",
    "#     config=config,\n",
    "#     stream_mode=\"values\",\n",
    "# ):\n",
    "#     step[\"messages\"][-1].pretty_print()\n",
    "\n",
    "\n",
    "response = graph.invoke(input_state, config)\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
=======
   "execution_count": null,
   "id": "be8fc7d7",
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> cc8911f44f43c92fd22450454f302d71d2c89db9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
